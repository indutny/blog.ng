<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Fedor Indutny's blog]]></title><description><![CDATA[Fedor Indutny's blog]]></description><link>http://darksi.de</link><generator>metalsmith-feed</generator><lastBuildDate>Fri, 24 Aug 2018 20:36:25 GMT</lastBuildDate><atom:link href="http://darksi.de/rss.xml" rel="self" type="application/rss+xml"/><author><![CDATA[Fedor Indutny]]></author><item><title><![CDATA[HashWick V8 Vulnerability]]></title><description><![CDATA[<p><img src="/images/hash-wick-small.png" alt="Hash Wick"></p>
<p>About one year ago, I&#39;ve discovered a way to do a Denial-of-Service (DoS) attack
on a local Node.js instance. The process involved sending huge amounts of data
to the HTTP server running on the same machine as the attacker, and measuring
the timing differences between various payloads. Given that the scope of attack
was limited to the same machine, it was decided by V8 team and myself that the
issue wasn&#39;t worth looking in yet. Nevertheless, a <a href="https://darksi.de/f.v8-hash-seed-timing-attack/">blog post</a> was published.</p>
<p>This year, I had a chance to revisit the Hash Seed guessing game with restored
enthusiasm and new ideas. The results of this experiment are murky, and no fix
is available yet in V8. Thus <strong>all V8 release lines are vulnerable to the
HashWick attack</strong>.</p>
<p><em>(Disclaimer: the issue was disclosed responsibly. This blog post is published
after more than 90 days since the initial report)</em></p>
<h2 id="what-is-a-hash-seed-">What is a Hash Seed?</h2>
<p>The Hash Seed is a random number that is used as an initial value for the
(non-cryptographic) hash functions inside of V8 instances. Such numbers are used
not only in V8 or other VMs, they&#39;re used in <a href="https://lwn.net/Articles/711167/">kernels</a>, <a href="https://github.com/antirez/redis/blob/cefe21d28a75f4fdbf24823ce42e777c2b9d5c6f/src/dict.c#L74">databases</a>, and
many different kinds of software.</p>
<p>The reason to have seeded hash functions is to prevent collision attacks on
hash maps (e.g. JavaScript objects/dictionaries). In ideal scenario, use of
random seed should make guessing the hash value of a string/number an impossible
endeavor.</p>
<p>Whenever a Node.js instance parses HTTP headers or JSON object, V8 has to create
a hash map for it. Each hash map is backed by a list (storage) of length
<code>2 * N</code>. The keys are inserted at even positions, the values are inserted at odd
(right after the key). The position index is determined by the hash of the key
modulo the storage size. Two equal keys will have two equal hashes, and will
point to the same cell in the list.</p>
<p>In ideal scenario the indices for two different keys are always different. It
is easy to see that it isn&#39;t possible due to the limited list size. The more
key/value pairs we insert into the object, the more likely the &quot;collision&quot; to
happen. When the hash values are the same, but the keys are different, V8
has to place the key in the next cell... or in the cell after the next, if the
next is filled already.</p>
<p>All in all, this provides quite optimal insertion/lookup performance at
relatively low memory costs.</p>
<h2 id="what-if-attacker-knows-hash-seed-">What if attacker knows Hash Seed?</h2>
<p>Every publicly exposed system is subject for external attacks. In the worst
case, an attacker gains access to the system or data in it. In less worse cases
the attacker might be able to overload the resources of the system by repeatedly
hitting the &quot;slow&quot; paths in the system&#39;s code. Such attacks are called &quot;Denial
of Service&quot; attacks or simply DoS.</p>
<p>For the hash maps in V8, the hash collisions are exactly the slow path. Being
able to generate a lot of (or precompute) keys with similar hash values (for
various hash map storage capacities) without using many resources is a guarantee
of success for such an attack. The worst case scenario is that a small JSON
object (or HTTP headers) could take seconds to parse. Sending thousands of such
objects would render any server unresponsive.</p>
<p>What stands in a way of generating the collisions is a random Hash Seed number.
Given sufficient size of that number and a strong hash function, the indices of
the different keys would look absolutely random. Guessing the Hash Seed becomes
impractical both time-wise and information-wise.</p>
<p>Unfortunately for V8, the hash function isn&#39;t strong enough and the hash seed
is a small number.</p>
<h2 id="how-to-find-v8-s-hash-seed-">How to find V8&#39;s Hash Seed?</h2>
<p>Imagine the following scenario: a Node.js HTTP server is available publicly and
accepts JSON bodies for POST requests. It&#39;d be reasonable to say that many
public Node.js servers are of this kind. It&#39;s important to note that this is
also applicable if Node.js is not directly exposed to a public internet port,
such as the case of being proxy-passed via Nginx.</p>
<p>How can an attacker find the value of V8&#39;s Hash Seed in such case?</p>
<p>This was a subject of my research this and last year. Assuming absence of access
to the server itself, the only way seems to be through the timing differences
between processing different crafted HTTP requests.</p>
<p>Hitting the slow path in hash map key insertion that we discussed above should
give a slightly slower response time. Knowing a lot of key combinations that
cause the slowdown (and as many combinations that doesn&#39;t cause one) is enough
for reconstructing the seed value. The problem is, the difference between slow
and fast path is in the order of microseconds, while the network latency is
often at least a couple of milliseconds. The network jitter overwrites any minor
timing differences without leaving us a chance to measure them.</p>
<p>What is needed for successful attack is an <strong>&quot;amplification&quot;</strong>. The request has
to be crafted in such way that the key insertion is triggered many times in a
sequence for the same key and same hash map contents. Each insertion would take
just a couple of microseconds, but a thousand of them would add up to a
millisecond!</p>
<p>Without giving away the complete tools for crafting such an attack, I can say
that the JSON body might look like this:</p>
<p><code>{&quot;100000&quot;:0,&quot;101&quot;:1,&quot;101&quot;:1,...repeat many times...}</code></p>
<p>It exploits several implementation details in V8 and a few quirks of JavaScript.
In particular, a JavaScript number lookup <code>obj[100000]</code> is equivalent to
looking up the &quot;stringified&quot; value of the same number <code>obj[&quot;100000&quot;]</code>.</p>
<p>The next quirk is that JSON objects might contain duplicate keys. Each key will
be parsed and looked up in the object, regardless of how many times it is
repeated in the JSON. <em>Each lookup triggers either slow or fast path in the
insertion code</em>.</p>
<p>The object above essentially becomes a sparse <code>Array</code> backed by a hash map with
very shallow storage capacity (just <code>4</code> values). Here&#39;s how V8 inserts key into
it:</p>
<ol>
<li>Initially <code>storage</code> is <code>[ null, null, null, null ]</code></li>
<li>The index of the first key (<code>100000</code>) is computed using the hash function and
the hash seed: <code>index = hash(100000, seed) % 4</code></li>
<li>The value <code>0</code> is inserted into that slot of storage (e.g. <code>index = 1</code>):
<code>[ null, 0, null, null ]</code></li>
<li>The index of the second key is computed
<code>index_2 = hash(random_key, seed) % 4</code></li>
<li><code>while (storage[index_2] != null) index_2++</code></li>
<li>The value is stored in <code>index_2</code> slot</li>
</ol>
<p>Step 5 is crucial for my attack. The extra check takes extra time, and this time
difference could be measured to get information about the hash seed.</p>
<p>Thus fixing the first key to <code>100000</code>, and trying out different keys
(probes) will give us the list of timings per each random key. Quarter of the
probes will have the same index as <code>100000</code> and will hit the slow path due to
collision. The rest will go through the fast path. The processing script would
later go through the list of timings and separate it by <code>75%</code> percentile of
latency. Most probes would be correctly classified as fast/slow. Misclassified
probes would increase Hash Seed search time.</p>
<p>The result of processing is a list of probe keys, each with an assigned class:
&quot;same index as 100000&quot; or &quot;different index&quot;. Each entry of this list is a
constraint on the Hash Seed value. An OpenCL script goes through all possible
32-bit Hash Seed values (<code>0 - 0x3fffffff</code>), and finds the one that satisfies
the majority of the constraints. That value is the most likely Hash Seed of the
attacked V8 instance. The OpenCL computation could take place on either CPU or
GPU, and completes the search in about one minute on my MacBook Pro.</p>
<p>My unoptimized PoC sends about 2 gb of JSON data to the server, collects the
timing information, and computes the Hash Seed in a <em>couple of minutes</em> (this
includes OpenCL brute-force time).</p>
<h2 id="prevention">Prevention</h2>
<p>This sounds dreary, but the fixes can be made. First of all, the hash seed size
has to be increased from 32 bits to 64 bits (this is already done in V8). Next,
the hash function has to be changed to <a href="https://en.wikipedia.org/wiki/SipHash">SipHash</a> or other hash function with
PRF (Pseudo-Random Function) properties.</p>
<p>Google has an amazing team working on V8. I&#39;m really hopeful that the remaining
fixes will be completed soon.</p>
<h2 id="prior-art">Prior-Art</h2>
<p>The hash collision attacks have <a href="https://lwn.net/Articles/474912/">a long history</a>, and most of the software
projects has moved to <a href="https://en.wikipedia.org/wiki/SipHash">SipHash</a> and at least 64-bit Hash Seed since then.</p>
<hr>
<h4 id="credits">Credits</h4>
<p>Huge thanks to <a href="https://github.com/aheckmann">Aaron Heckmann</a>, <a href="https://github.com/snowinferno">Greg Wilburn</a>, <a href="https://github.com/rauchg">Guillermo Rauch</a>, and
<a href="https://github.com/shaunwarman">Shaun Warman</a> for providing feedback on this article.</p>
]]></description><link>http://darksi.de/12.hashwick-v8-vulnerability</link><guid isPermaLink="true">http://darksi.de/12.hashwick-v8-vulnerability</guid><dc:creator><![CDATA[Fedor Indutny]]></dc:creator><pubDate>Wed, 22 Aug 2018 00:00:00 GMT</pubDate></item><item><title><![CDATA[HyperBloom]]></title><description><![CDATA[<p>Over this weekend I got not so original (but definitely a fun one) idea to build
fully distributed and decentralized Twitter. At the time it was inspired by the
<a href="https://datproject.org/">DAT Project</a> and <a href="https://github.com/mafintosh/hypercore">Hypercore</a>, neither of which could support public
replies to user feeds.</p>
<p>Hence, the most natural thing was to <a href="https://xkcd.com/927/">write a new protocol</a>! Say hello to
<a href="https://github.com/hyperbloom/hyperbloom">HyperBloom</a>!</p>
<h2 id="protocol">Protocol</h2>
<p>It is crucial to understand the needs for the protocol before discussing the
protocol itself. Let me list few requirements for it:</p>
<ul>
<li>Decentralized and distributed</li>
<li>Viral. Everyone can reply to anyone&#39;s tweet without exchanging any public
keys or information ahead of time</li>
<li>Secure</li>
</ul>
<p>How could it combine all these three qualities into one protocol? By combining
existing solutions, of course:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type#State-based_grow-only_set">State-based grow-only set</a> with <a href="https://en.wikipedia.org/wiki/Bloom_filter">Bloom Filters</a> for diffs</li>
<li>Distributed <a href="https://en.wikipedia.org/wiki/Public_key_infrastructure">Public Key Infrastructure</a> (PKI) for append permissions</li>
</ul>
<h3 id="trust-network">Trust Network</h3>
<p>Having grow-only set that is writable by anyone on the web is the best way to
introduce enormous amount of spam into social network. This can be tackled by
accepting writes only from your friends. However, this kills the virality of the
platform.</p>
<p>Perhaps friends-of-friends should be allowed to append to that Set? Better!</p>
<p>The way <a href="https://github.com/hyperbloom/hyperbloom">HyperBloom</a> addresses this is by letting author&#39;s issue so called
<a href="https://github.com/hyperbloom/hyperbloom-protocol/blob/master/spec.md#signature-chain"><em>Trust Links</em></a>. Each <em>Trust Link</em> acts like an edge in the Graph: <code>A -&gt; B</code>
or _A_ trusts _B_. When two peers connect to synchronize the values in a Set,
they each present a collection of links from the author of the Set to the peers
themselves. Each successive link in such collection is a continuation of the
previous link: <code>A -&gt; B, B -&gt; C, C -&gt; D</code>. With a total limit of <strong>5</strong> links in
one collection (chain).</p>
<p>The limit is imposed to save the bandwidth. To further save it peers help each
other by automatically issuing links that create shorter path to the author.</p>
<p>Example:</p>
<ol>
<li>Peer B has following chain: <code>A -&gt; B</code></li>
<li>Peer C has following chain: <code>A -&gt; D, D -&gt; E, E -&gt; C</code></li>
<li>Peer B sends <code>B -&gt; C</code> to C to minimize the route</li>
<li>Peer C uses <code>A -&gt; B, B -&gt; C</code> later on</li>
</ol>
<p>Each link has an expiration time, and such automatic link as in the example will
have the expiration time set to: <code>minimum(A -&gt; D, D -&gt; E, E -&gt; C)</code>. Essentially,
<code>A</code> always controls how often it wants to refresh its peers&#39; trust.</p>
<h3 id="set">Set</h3>
<p>Set is not particularly interesting. It borrows some design decisions from
<a href="https://bitcoin.org/en/glossary/simplified-payment-verification">Bitcoin SPV client</a>. In particular the use of Bloom filters to optimally
compute the difference between the peers and set the missing values.</p>
<h2 id="usage">Usage</h2>
<p>I&#39;m combining both Hypercore and HyperBloom into a project called (for no
particular reason. May be Hyper-uni-corn?) <a href="https://github.com/indutny/hypercorn">HyperCorn</a>. HyperCorn uses
HyperCore logs for JSON message fields, and HyperBloom for notifying authors
about replies to their feeds.</p>
<p>HyperBloom Trust Links are stored and distributed through HyperCore. So far it
appears to be working, but it has pretty long way to go still. Mainly it needs
an UI. Contact me, if you are interested!</p>
<h2 id="open-questions">Open Questions</h2>
<ul>
<li>Is virality a good thing?</li>
<li>Should auto-links be issued?</li>
<li>Is 5 links enough?</li>
</ul>
<p>I&#39;d love to hear your opinion.</p>
<p>(You can reply on <a href="https://twitter.com/indutny/status/857136827639189504">twitter</a>)</p>
<p>Thanks for reading.</p>
]]></description><link>http://darksi.de/11.hyperbloom</link><guid isPermaLink="true">http://darksi.de/11.hyperbloom</guid><dc:creator><![CDATA[Fedor Indutny]]></dc:creator><pubDate>Wed, 26 Apr 2017 00:00:00 GMT</pubDate></item><item><title><![CDATA[Bitcoin to the Moon]]></title><description><![CDATA[<p>Behold, this is rather unusual post for this blog. Instead of exploring
vastness of technical wonders, this post will concentrate on discussion and
explanation of recent events in Bitcoin community.</p>
<h2 id="prerequisites">Prerequisites</h2>
<ul>
<li><a href="https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-April/013996.html">Greg Maxwell&#39;s email</a></li>
<li><a href="https://github.com/tothemoon-org/extension-blocks">To the Moon proposal</a></li>
</ul>
<h2 id="statement">Statement</h2>
<p>Recently I got involved in the <a href="https://github.com/tothemoon-org/extension-blocks">initiative</a> related to my past interest in
Bitcoin technology. This effort is lead by <a href="https://github.com/chjj">Christopher Jeffrey (JJ)</a> from
<a href="https://purse.io/">purse.io</a>, <a href="https://twitter.com/jcp?lang=en">Joseph Poon</a> from <a href="http://lightning.network/">Lightning Network</a>, <a href="https://twitter.com/spair">Stephen Pair</a>
from <a href="https://bitpay.com/">bitpay.com</a>. To make it clear from the start, my contribution to this
project was rather small and limited to providing superficial technical review.</p>
<p>The <a href="https://github.com/tothemoon-org/extension-blocks">initiative</a> is based off the <a href="https://bitcointalk.org/index.php?topic=283746.0">Auxillary Block proposal</a> that is
slightly older than the <a href="https://github.com/bcoin-org/bcoin">bcoin</a> project. It was modified to
accommodate the use of Lightning, and (although, it is incompatible with
<a href="https://github.com/bitcoin/bips/blob/master/bip-0141.mediawiki">BIP 141</a>) Segregated Witness (<em>SEGWIT</em>).</p>
<p>The main advantage over Segregated witness is that the block size is no longer
tied to the number of transactions in it. Many transactions could be stored
outside of the canonical blocks in so called Extension blocks. There are pros
and cons to this approach that better be discussed on <a href="https://github.com/tothemoon-org/extension-blocks">github</a>.</p>
<p>The most important thing is that it attempts to solve the contention between
miners and bitcoin core. It has been known for some time that many miners were
willing to switch to <a href="https://www.bitcoinunlimited.info/">Bitcoin Unlimited</a> (_BU_) (or in other words do a
hard-fork) because of the block size limitation. Not only _BU_ is a hard fork,
but it also prone to <a href="http://www.coindesk.com/code-bug-exploit-bitcoin-unlimited-nodes/">bugs</a> that already brought it down once. Obviously,
given this information one has to be very careful when considering it.
<a href="https://github.com/bitcoin/bips/blob/master/bip-0141.mediawiki">SEGWIT</a>, on other hand, is a fantastic effort and a soft-fork alternative to
_BU_. However it still caps the block size, and thus is not acceptable for
miners for the very same reason as the current version of Bitcoin.</p>
<p>Given this context, <a href="https://github.com/tothemoon-org/extension-blocks">a proposal</a> has been created to address this once and
for all. Purse.io has <a href="https://medium.com/purse-essays/extension-block-story-619a46b58c24">reached out</a> to the companies involved in Bitcoin to
collect their feedback and feature requests. All in all, it appears to me that
&quot;To the Moon&quot; proposal should address the needs of everyone without much
compromise of our ideals and Bitcoin ideology.</p>
<h2 id="asicboost">ASICBOOST</h2>
<p>Almost at the same time as &quot;To the Moon&quot; proposal was published, Greg Maxwell
has sent an <a href="https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-April/013996.html">email</a> about <a href="https://www.asicboost.com/">ASICBOOST</a> and his findings about usage of
this optimization in &quot;particular mining chip&quot; (quoting Greg).</p>
<p>Given the timing this email has sprawled the discussion of miner&#39;s reasons to
&quot;love&quot; <a href="https://github.com/tothemoon-org/extension-blocks">&quot;To the Moon&quot;</a> and &quot;hate&quot; <a href="https://github.com/bitcoin/bips/blob/master/bip-0141.mediawiki">BIP 141</a>. It turned out that
&quot;To the Moon&quot; was compatible with ASICBOOST&#39;s optimization, while <em>SEGWIT</em> is
not.</p>
<p>I can&#39;t resist diving into the technical details of this optimization, but
before I&#39;ll walk this road with you let me quickly reassure you. As of
<a href="https://github.com/tothemoon-org/extension-blocks/commit/5331eeed1880ecc43a250313415e0d0b02c56bab">this commit</a> &quot;To the Moon&quot; is no longer compatible with ASICBOOST, and,
although it is an open question whether this kind of optimization is
permissible or not, this proposal is on the same grounds with <em>SEGWIT</em> now.</p>
<p><strong>This means that reasons for conspiracy about relationship between
&quot;To the Moon&quot; and miners no longer holds.</strong></p>
<p>If you have any additional prevention measures in mind - please do not hesitate
to open an issue on <a href="https://github.com/tothemoon-org/extension-blocks">github</a>.</p>
<h2 id="technical-details-">Technical details!</h2>
<p>Finally :)</p>
<p>The most of the content below relies on some understanding of
<a href="https://arxiv.org/pdf/1604.00575.pdf">this ASICBOOST paper</a>. This paper is not to hard to follow, so please take
a look.</p>
<p>It is a normal practice in Bitcoin mining to pre-compute as much as possible to
make mining possible. ASIC&#39;s mostly do double SHA256 hashes, thus this
pre-computation relies heavily on splitting SHA256 into phases and sharing data
for inputs that do not change.</p>
<p>The way Bitcoin is mined is by brute-forcing the 32-bit nonce in the block
header until the hash of the header will match current complexity of the
Blockchain (read, number of leading zeroes in the hash). Trying all 32 bits is
of course not enough to generate such fancy looking block hashes, and almost in
every case some additional modifications to the block header are needed.</p>
<p>Given the structure of the block header:</p>
<pre><code>02000000 ........................... Block version: 2

b6ff0b1b1680a2862a30ca44d346d9e8
910d334beb48ca0c0000000000000000 ... Hash of previous block&#39;s header
9d10aa52ee949386ca9385695f04ede2
70dda20810decd12bc9b048aaab31471 ... Merkle root

24d95a54 ........................... Unix time: 1415239972
30c31b18 ........................... Target: 0x1bc330 * 256**(0x18-3)
fe9f0864 ........................... Nonce
</code></pre><p>(<a href="https://bitcoin.org/en/developer-reference#block-headers">Source of the data</a>)</p>
<p>The only field that miners has control of (other than timestamp, which can&#39;t
be changed too much for obvious reasons) is root of the <a href="https://en.wikipedia.org/wiki/Merkle_tree">Merkle tree</a> with
block&#39;s transactions (_TX_) as leafs. This is usually approached by modifying
the first _TX_ (coinbase) in the block.</p>
<p>Now as the Merkle root changes - it will practically invalidate any SHA256
pre-computation that could have been made, since the changes will span both of
the two 64-byte chunks (including padding) forming the 80-byte block header.
(SHA256 operates on 64-byte chunks).</p>
<p>What can one do about it? Second 64-byte chunk starts from the last 4 bytes of
the merkle root... Does it ring the bell yet?</p>
<p>The answer is: collisions!</p>
<p>The pre-computation is still partially possible if the second 64-byte chunk is
the same during mining. One doesn&#39;t have to keep it always the same, generating
few of such colliding chunks is enough to get the benefits of the optimization.</p>
<p>Now how this collision may be generated? The answer is <a href="https://en.wikipedia.org/wiki/Brute-force_attack">brute-force</a>.</p>
<p>(The rest of the post is basically elaboration upon <a href="https://lists.linuxfoundation.org/pipermail/bitcoin-dev/2017-April/013996.html">Greg&#39;s email</a> which I
hope you already checked).</p>
<p>How many brute-force attempts has to be done? Applying <a href="https://en.wikipedia.org/wiki/Birthday_problem">Birthday Paradox</a>
gives the number of tries around <code>2^16</code> (size of whole problem space is <code>2^32</code>,
since we need to collide just 4 bytes) for two colliding block headers. Four of
them will take approximately <code>2^24</code> tries. Quite a lot, but not too much if you
can optimize it. Let&#39;s now consider how this brute-force could work.</p>
<p>The most straightforward way of doing it would be changing the coinbase, but
this is rather expensive for regular blocks. If block has around <code>1500</code> TXs this
means re-computing the Merkle tree branch of length <code>10</code>. Thus <code>10</code>
double-SHA256 per brute-force attempt. Very inefficient!</p>
<p>Can we do less? Yes - we can!</p>
<p>Transactions can be re-arranged in the block to change the Merkle root, each
different permutation will yield a different hash and thus will count as a try.
Permuting 7 branches to the right of the Merkle root yields <code>5040</code> combinations
with <code>7</code> double-SHA526 hashes per try. Changing the coinbase to the left of the
root can produce <code>4096</code> more combinations. Combining these two together gives
us just <code>2^24</code> tries that we was looking for! Now since we pre-cached various
choices for both left and right branches the only thing that is left is compute
double-SHA256 of both of them for every combination. To conclude: just <code>1</code>
double-SHA256 per try! Now this sounds quite good.</p>
<h2 id="segwit-and-to-the-moon">SEGWIT and &quot;To the Moon&quot;</h2>
<p>This optimization is possible with &quot;classical&quot; Bitcoin, but is not feasible with
SEGWIT for a very simple reason. Coinbase in SEGWIT includes the Merkle root
over the rest of transactions (ignoring technical details), which means that
it is not possible to combine left and right branches without changing the
coinbase which brings us back to <code>10</code> double-SHA256 per try.</p>
<p>Initial version of &quot;To the Moon&quot; has a Merkle tree in coinbase too, but it
had to be computed only over transactions that are not present in
classical/canonical block. Meaning that it is easier to do ASICBOOST
optimization on &quot;To the Moon&quot; than on SEGWIT.</p>
<p>Given this description, it is easy to see that <a href="https://github.com/tothemoon-org/extension-blocks/commit/5331eeed1880ecc43a250313415e0d0b02c56bab">the recent change</a> in
&quot;To the Moon&quot; spec makes it non-susceptible to ASICBOOST optimization.</p>
<p>I hope this answers all or at least some of your questions about it.</p>
<p>Thank you for reading!</p>
<h3 id="credits">Credits</h3>
<p>I&#39;d like to thank:</p>
<ul>
<li>Greg Maxwell for doing a quick review of the change to &quot;To the Moon&quot; proposal,
and for helping me understand the details of his discovery</li>
<li>Christopher Jeffrey (JJ) for inviting me to the initiative</li>
<li>Guillermo Rauch for reviewing/proof-reading this post.</li>
</ul>
]]></description><link>http://darksi.de/10.bitcoin-to-the-moon</link><guid isPermaLink="true">http://darksi.de/10.bitcoin-to-the-moon</guid><dc:creator><![CDATA[Fedor Indutny]]></dc:creator><pubDate>Thu, 06 Apr 2017 00:00:00 GMT</pubDate></item><item><title><![CDATA[V8 hash seed timing attack]]></title><description><![CDATA[<h2 id="moment-of-history">Moment of History</h2>
<p>There is a mostly forgotten <a href="https://github.com/nodejs/node-v0.x-archive/issues/2431">security issue</a> that was fixed in
Node.js back in 2012. It was originally announced on the
<a href="https://www.youtube.com/watch?v=R2Cq3CLI6H8">28c3 conference</a> December, 2011 and the final fix landed in
<a href="https://github.com/nodejs/node/commit/16953329413831b32f4c3b2255aeacec874ed69d">January, 2012</a>.</p>
<p>In few words, the most of dynamic languages use either bucket lists or
<a href="https://en.wikipedia.org/wiki/Hash_table#Open_addressing">open addressing</a> variants of hash tables. V8 uses the latter one, and in
such case when VM is asked to insert a property into an object it does
the following sequence of actions:</p>
<ol>
<li>Compute the hash of the key (quite often with a <a href="https://en.wikipedia.org/wiki/Jenkins_hash_function">Jenkins hash</a>)</li>
<li>Clear the high bits of the hash value</li>
<li>Use it as an index in the internal array</li>
<li>Find unused slot in that array</li>
<li>Insert the key/value pair at that slot.</li>
</ol>
<p>This sounds pretty much OK, except for the step 4. One may ask: What if the
target slot is way too far from the index in the step 3? The answer is: it will
take more time to do such insertion.</p>
<p>Do you see where it is going?</p>
<h2 id="collision-attacks">Collision attacks</h2>
<p>If the attacker can insert many keys like these into the hash table - the whole
procedure is going be much slower than usual (20x slower in some cases). During
this time Node.js will be blocked, and performing such insertions one after
another leads directly to Denial of Service attack. To put it in concrete
context: <code>req.headers</code> in <code>http.Server</code> is populated with user data, and is thus
susceptible to this kind of attack.</p>
<p>How does one generate such keys? Trivial and fast brute-force could generate as
many keys as needed to give desired &quot;collisions&quot; given that attacker knows what
hash function is used by VM.</p>
<p>What was done to fix it in V8/Node.js? Together with V8 team we added seeds to
all hash tables used in V8, and made sure that they are randomized on the
process start.</p>
<h2 id="inspiration-for-an-experiment">Inspiration for an Experiment</h2>
<p>After reading this <a href="http://perl11.org/blog/seed.html">Perl blog post</a> I thought that it would be funny to
carry out an actual hash seed extraction out of the live node.js process:
first - locally within the process itself, second - from http server on the same
machine, third - remotely (no luck so far). Knowing the seed means being able
to craft the collisions, and this gets us back to DoS problem.</p>
<p>Numerous code paths in V8 have been tried, until I stumbled upon a
<a href="https://github.com/v8/v8/blob/140d4df7954259e60a555efc0b2d00a9c924564c/src/objects-inl.h#L3140-L3157">particular function</a>. There V8 puts a new property into an internal list
called <code>DescriptorArray</code>. For performance reasons properties in that array
must be sorted, and since V8 extends the array - it has to shift all bigger
properties to the right to make space for the new one.</p>
<p>By measuring timing of such insertions, attacker could figure out approximate
position of the inserted key. <code>DescriptorArray</code> holds no more than 18 keys, so
it could be attempted to insert the same 17 keys and one random one many times,
and infer the difference in timing to find the random keys that were placed
either at the very end or at the start of the array.</p>
<p>It&#39;s easier said than done, though. V8 has many layers of caching (which is one
of the reasons why your JavaScript code is so fast!). In order to get
through to that <code>DescriptorArray::Append</code> function, I had to outflank all of
them. In the end, the resulting program does everything in reverse - the random
key is inserted first, and then 17 predefined keys are inserted right after it.
The difference is non-obvious, but that&#39;s the part of the solution to skip all
of the caches.</p>
<p>This is how it looks:</p>
<pre><code class="lang-javascript">function test(pre, list) {
  const o = {};
  o[pre] = null;
  for (let i = 0; i &lt; list.length; i++)
    o[list[i]] = null;
  return o;
}
</code></pre>
<p>Now this script has to create a <code>list</code> of keys, and a large number of probes
(2093 strings that are passed one after another as <code>pre</code>). It can try each probe
with the same <code>list</code>, and measure the timing with <code>process.hrtime()</code> with
nanosecond precision. Largertime difference means that the probe was inserted
at the start of the <code>DescriptorArray</code>, and thus its hash value (32 bit number)
is less than all hashes of the keys in the <code>list</code>. When the time delta is least</p>
<ul>
<li>probe was appended to the end of the <code>DescriptorArray</code>, meaning that its hash
is the biggest in it.</li>
</ul>
<p>It may sound like not too much to stick to, but if one can collect enough such
&quot;relations&quot; between the probe and keys - one can brute force all 32-bit seed
values to find the one that gives the best approximation to this relation!
In fact, it takes just about 15 minutes on 20 core machine to do it, and this
time can be improved by using GPU (I dare you!).</p>
<p>The most important part of brute forcing function is <code>check</code>:</p>
<pre><code class="lang-c">static int check(uint32_t seed) {
  int score;
  uint32_t key_hashes[ARRAY_SIZE(keys)];

  score = 0;

  for (size_t i = 0; i &lt; ARRAY_SIZE(keys); i++) {
    key_hashes[i] = jenkins(keys[i], seed);
  }

  for (size_t i = 0; i &lt; ARRAY_SIZE(probes); i += 2) {
    uint32_t l;
    uint32_t r;

    l = jenkins(probes[i], seed);
    r = jenkins(probes[i + 1], seed);

    for (size_t j = 0; j &lt; ARRAY_SIZE(keys); j++) {
      if (l &lt; key_hashes[j])
        score++;
      if (key_hashes[j] &lt;= r)
        score++;
    }
  }

  return score;
}
</code></pre>
<p><code>l</code> is a probe that supposedly has the lowest hash value in <code>DescriptorArray</code>,
<code>r</code> is a probe that has the highest. So all in all, <code>brute.c</code> just searches for
the seed value that maximizes the result of <code>check</code>. Simple!</p>
<h2 id="conclusions-and-code">Conclusions and code</h2>
<p>This works locally as a charm, and works with a <code>http.Server</code> on the same
machine to some extent. Unfortunately (or fortunately?) this attack doesn&#39;t work
with remote servers (or I wasn&#39;t able to execute it), even when the ping is
around 1ms. The timing difference that we are looking for is about several
microseconds, and it looks like it is smudged out in the network delay
distribution.</p>
<p>V8 team is aware of this effort, and the decision is that this is not a security
defect - hence it is published here.</p>
<p>All code is available on <a href="https://github.com/indutny/hash-cracker">github</a>. Please enjoy with care!</p>
]]></description><link>http://darksi.de/f.v8-hash-seed-timing-attack</link><guid isPermaLink="true">http://darksi.de/f.v8-hash-seed-timing-attack</guid><dc:creator><![CDATA[Fedor Indutny]]></dc:creator><pubDate>Thu, 19 Jan 2017 00:00:00 GMT</pubDate></item><item><title><![CDATA[uv_link_t - libuv pipeline]]></title><description><![CDATA[<h2 id="preface">Preface</h2>
<p>Writing servers/clients in C could be non-trivial. Even with the help of such
powerful (and awesome dinosaur) libraries as <a href="https://github.com/libuv/libuv">libuv</a>, it still takes lots of
effort and boilerplate code to create real world applications.</p>
<p>Some of this boilerplate code comes from the use of the widespread protocols
like TLS (SSL) and HTTP. While there are popular implementations available
as an Open Source libraries (<a href="https://github.com/openssl/openssl">OpenSSL</a>, <a href="https://github.com/nodejs/http-parser">http-parser</a>), they still either
provide very abstract interface (like <a href="https://github.com/nodejs/http-parser">http-parser</a>), or an API to transfer
the responsibility of the networking to the library itself (like <code>SSL_set_fd()</code>
in <a href="https://github.com/openssl/openssl">OpenSSL</a> and Amazon&#39;s <a href="https://github.com/awslabs/s2n">s2n</a>). Such abstract nature makes them easier
to embed, but the adaptor code inevitably tend to appear in the particular
applications.</p>
<h2 id="precursor-streambase">Precursor - StreamBase</h2>
<p><a href="https://github.com/libuv/libuv">libuv</a> is hardly an exception, and <a href="https://github.com/nodejs/node/blob/master/src/tls_wrap.cc">node.js</a> and <a href="https://github.com/indutny/bud/blob/master/src/client.c">bud</a>&#39;s TLS
implementation is a vivid evidence of this. However, in a contrast to <a href="https://github.com/indutny/bud/blob/master/src/client.c">bud</a>,
<a href="https://github.com/nodejs/node/blob/master/src/tls_wrap.cc">node.js</a> TLS code lives off on an abstraction called <a href="https://github.com/nodejs/node/blob/master/src/stream_base.h">StreamBase</a>. By
separating <a href="https://github.com/libuv/libuv">libuv</a>-specific adaptor code into a generic C++ class, we have
created a foundation for a simpler and reusable implementation of any other
protocol! See, for example, recent <a href="https://github.com/nodejs/node/blob/29228c4089431d0e65749421f43aafd05694f376/src/node_http_parser.cc#L472-L486">node_http_parser.cc</a> which uses only
a minor amount of power available through the means of <a href="https://github.com/nodejs/node/blob/master/src/stream_base.h">StreamBase</a>, but
nevertheless provides <a href="https://github.com/nodejs/node/pull/2355">10-20%</a> performance improvement since its inception.</p>
<p>This implementation has some major drawbacks, preventing its wider adoption
outside of the node.js core:</p>
<ul>
<li>C++ headers: lots of virtual classes, complex API, non-trivial inheritance
scheme</li>
<li>High internal dependence on the node.js core itself</li>
</ul>
<p>Because of these issues (and my own limitations) <a href="https://github.com/nodejs/node/blob/master/src/stream_base.h">StreamBase</a> has defied all
attempts to make it public.</p>
<h2 id="uv_link_t">uv_link_t</h2>
<p>Heavily inspired by the success of <a href="https://github.com/nodejs/node/blob/master/src/stream_base.h">StreamBase</a> in the node.js core, a
<a href="https://github.com/indutny/uv_link_t">uv_link_t</a> library was created. It has lots of similarities with the
<a href="https://github.com/nodejs/node/blob/master/src/stream_base.h">StreamBase</a>, but it is:</p>
<ul>
<li>Implemented in C: self-documented structures, C-cast based inheritance, etc</li>
<li>Standalone library</li>
</ul>
<p>The API is based on the <a href="http://docs.libuv.org/en/v1.x/stream.html">uv_stream_t</a> and shouldn&#39;t come as a big surprise
to the users, since <a href="https://github.com/indutny/uv_link_t">uv_link_t</a> is intended to be used together with
<a href="https://github.com/libuv/libuv">libuv</a>.</p>
<p>Here is a visual explanation of how <a href="https://github.com/libuv/libuv">uv_link_t</a> works:</p>
<p><img src="/images/uv_link_source_t.svg" alt="uv_link_source_t"></p>
<h2 id="examples">Examples</h2>
<p>Before we take a peek at the APIs, let&#39;s discuss what can be done with
<a href="https://github.com/indutny/uv_link_t">uv_link_t</a>. Technically, any stream-based (i.e. anything that uses
<code>uv_stream-t</code>) protocol can be implemented on top of it. Multiple protocols can
be chained together (that&#39;s why it is called <code>uv_</code><strong>link</strong><code>_t</code>!), provided that
there is an implementation:</p>
<p><code>TCP &lt;-&gt; TLS &lt;-&gt; HTTP &lt;-&gt; WebSocket</code>.</p>
<p>This chaining works in a pretty transparent way, and every segment of it can be
observed without disturbing the data flow and operation of the other links.</p>
<p>Existing protocols:</p>
<ul>
<li><a href="https://github.com/indutny/uv_ssl_t">uv_ssl_t</a> - TLS, based on OpenSSL&#39;s API</li>
<li><a href="https://github.com/indutny/uv_http_t">uv_http_t</a> - low-level HTTP/1.1 implementation, possibly incomplete</li>
</ul>
<p>Small demo-project:</p>
<ul>
<li><a href="https://github.com/indutny/file-shooter">file-shooter</a> - dumb-simple HTTPS server based on both <a href="https://github.com/indutny/uv_ssl_t">uv_ssl_t</a> and
<a href="https://github.com/indutny/uv_http_t">uv_http_t</a></li>
</ul>
<p>Note that all these projects, including <a href="https://github.com/indutny/uv_link_t">uv_link_t</a> itself are supposed to
be built with a <a href="https://github.com/gypkg/gypkg">gypkg</a>, which is a subject for a future blog post.</p>
<h2 id="api">API</h2>
<p>The backbone of the API is a <code>uv_link_t</code> structure:</p>
<pre><code class="lang-c">#include &quot;uv_link_t.h&quot;

static uv_link_methods_t methods = {
  /* To be discussed below */
};

void _() {
  uv_link_t link;

  uv_link_init(&amp;link, &amp;methods);

  /* ... some operations */
  uv_link_close(&amp;link, close_cb);
}
</code></pre>
<p>In the most of the cases a first link should be an <code>uv_link_source_t</code>. It
consumes an instance of <code>uv_stream_t</code>, and propagates reads and writes from
the whole chain of links connected to it.</p>
<pre><code class="lang-c">uv_link_source_t source;

uv_stream_t* to_be_consumed;
uv_link_source_init(&amp;source, to_be_consumed);
</code></pre>
<p>As mentioned before, links can be chained together:</p>
<pre><code class="lang-c">uv_link_t a;
uv_link_t b;

/* Initialize `a` and `b` */
uv_link_chain(/* from */ a, /* to */ b);
</code></pre>
<p>This <code>uv_link_chain</code> call means that the data emitted by <code>a</code> will be passed as
an input to <code>b</code>, and the output of <code>b</code> will written to <code>a</code>.</p>
<p>Speaking of input/output, the API is pretty similar to <a href="https://github.com/libuv/libuv">libuv</a>&#39;s:</p>
<pre><code class="lang-c">int uv_link_write(uv_link_t* link, const uv_buf_t bufs[],
                  unsigned int nbufs, uv_stream_t* send_handle,
                  uv_link_write_cb cb, void* arg);

int uv_link_read_start(uv_link_t* link);
int uv_link_read_stop(uv_link_t* link);

void fn() {
  link-&gt;alloc_cb = /* something */;
  link-&gt;read_cb = /* something */;
}
</code></pre>
<p>Please check the <a href="https://github.com/indutny/uv_link_t/blob/master/docs/api.md">API docs</a> for further information on particular methods
and structures (likes <code>uv_link_source_t</code> and <code>uv_link_observer_t</code>).</p>
<p>There is also an <a href="https://github.com/indutny/uv_link_t/blob/master/docs/implementation-guide.md">Implementation guide</a> for implementing custom types of
<code>uv_link_t</code>.</p>
<h2 id="error-reporting">Error reporting</h2>
<p>Having multiple independent implementations of <code>uv_link_t</code> interface, it is a
natural question to ask: how does <code>uv_link_t</code> handle error code conflict?</p>
<p>The answer is that all error codes returned by <code>uv_link_...</code> methods are
actually prefixed with the index of the particular link in a chain. Thus, even
if there are several similar links in a chain, it is possible to get the pointer
to the <code>uv_link_t</code> instance that have emitted it:</p>
<pre><code class="lang-c">int uv_link_errno(uv_link_t** link, int err);
const char* uv_link_strerror(uv_link_t* link, int err);
</code></pre>
<h2 id="foreword-gypkg">Foreword: gypkg</h2>
<p><a href="https://github.com/gypkg/gypkg">gypkg</a> is recommended to be used when embedding <code>uv_link_t</code> in the C
project. There are not too many source files to put into a <code>Makefile</code> or some
other build file, but the convenience that <a href="https://github.com/gypkg/gypkg">gypkg</a> provides, pays off very
quickly!</p>
<h3 id="installation-node-js-v6-is-required-">Installation (node.js v6 is required):</h3>
<pre><code class="lang-sh">npm install -g gypkg
</code></pre>
<h3 id="init">Init</h3>
<pre><code class="lang-sh">mkdir project
cd project
gypkg init
</code></pre>
<h3 id="adding-uv_link_t-as-a-dependency">Adding <code>uv_link_t</code> as a dependency</h3>
<pre><code class="lang-sh">vim project.gyp
</code></pre>
<pre><code class="lang-python">{
  &quot;variables&quot;: {
    &quot;gypkg_deps&quot;: [
      &quot;git://github.com/libuv/libuv.git@^1.9.0 =&gt; uv.gyp:libuv&quot;,
      &quot;git://github.com/indutny/uv_link_t@^1.0.0 [gpg] =&gt; uv_link_t.gyp:uv_link_t&quot;,
    },
  },

  # Some other GYP things
}
</code></pre>
<h3 id="building">Building</h3>
<pre><code class="lang-sh">gypkg build
ls -la out/Release
</code></pre>
]]></description><link>http://darksi.de/e.uv-link-t</link><guid isPermaLink="true">http://darksi.de/e.uv-link-t</guid><dc:creator><![CDATA[Fedor Indutny]]></dc:creator><pubDate>Mon, 15 Aug 2016 00:00:00 GMT</pubDate></item><item><title><![CDATA[Sea of Nodes]]></title><description><![CDATA[<h2 id="brief-intro">Brief intro</h2>
<p>This post is going to be about the sea-of-nodes compiler concept that I have
recently learned.</p>
<p>While it is not completely necessary, it may be useful to take a peek at the
some of my previous posts on JIT-compilers before reading this:</p>
<ul>
<li><a href="/4.how-to-start-jitting">How to start JIT-ting</a></li>
<li><a href="/5.allocating-numbers">Allocating numbers</a></li>
<li><a href="/6.smis-and-doubles">SMIs and Doubles</a></li>
<li><a href="/a.deoptimize-me-not">Deoptimize me not, v8</a></li>
</ul>
<h2 id="compilers-translators">Compilers = translators</h2>
<p>Compilers are something that every Software Engineer uses several times a day.
Surprisingly even people who consider themselves to be far from writing the
code, still use a compiler quite heavily throughout their day. This is
because most of the web depends on client-side code execution, and many of such
client-side programs are passed to the browser in a form of the source code.</p>
<p>Here we come to an important thing: while source code is (usually)
human-readable, it looks like complete garbage to your
laptop/computer/phone/...&#39;s CPU. On other hand, machine code, that computers
<strong>can</strong> read, is (almost always) not human-readable. Something should be done
about it, and the solution to this problem is provided by the process called
<strong>translation</strong>.</p>
<p>Trivial compilers perform a single pass of <em>translation</em>: from the source code
to the machine code. However, in practice most compilers do at least two passes:
from the source code to <a href="https://en.wikipedia.org/wiki/Abstract_syntax_tree">Abstract Syntax Tree</a> (AST), and from <a href="https://en.wikipedia.org/wiki/Abstract_syntax_tree">AST</a> to
machine code. <a href="https://en.wikipedia.org/wiki/Abstract_syntax_tree">AST</a> in this case acts like an <em>Intermediate Representation</em>
(IR), and as the name suggests, <a href="https://en.wikipedia.org/wiki/Abstract_syntax_tree">AST</a> is just another form of the same source
code. These intermediate representations chain together and essentially are
nothing else but the abstraction layers.</p>
<p>There is no limit on the layer count. Each new layer brings the source program
closer to how it will look like in machine code.</p>
<h2 id="optimization-layers">Optimization layers</h2>
<p>However, not all layers are used solely for translation. Many compilers also
additionally attempt to optimize the human-written code. (Which is usually
written to have a balance between code elegance and code performance).</p>
<p>Take the following JavaScript code, for example:</p>
<pre><code class="lang-javascript">for (var i = 0, acc = 0; i &lt; arr.length; i++)
  acc += arr[i];
</code></pre>
<p>If the compiler would translate it to the machine code straight out of <a href="https://en.wikipedia.org/wiki/Abstract_syntax_tree">AST</a>,
it may resemble (in very abstract and detached from reality instruction set):</p>
<pre><code>acc = 0;
i = 0;
loop {
  // Load `.length` field of arr
  tmp = loadArrayLength(arr);
  if (i &gt;= tmp)
    break;

  // Check that `i` is between 0 and `arr.length`
  // (NOTE: This is necessary for fast loads and
  // stores).
  checkIndex(arr, i);

  // Load value
  acc += load(arr, i);

  // Increment index
  i += 1;
}
</code></pre><p>It may not be obvious, but this code is far from optimal. Array length does not
really change inside of the loop, and the range checks are not necessary at all.
Ideally, it should look like this:</p>
<pre><code>acc = 0;
i = 0;
len = loadArrayLength(arr);
loop {
  if (i &gt;= tmp)
    break;

  acc += load(arr, i);
  i += 1;
}
</code></pre><p>Let&#39;s try to imagine how we could do this.</p>
<p>Suppose we have an <a href="https://en.wikipedia.org/wiki/Abstract_syntax_tree">AST</a> at hand, and we try to generate the machine code
straight out of it:</p>
<p><em>(NOTE: Generated with <a href="https://github.com/jquery/esprima">esprima</a>)</em></p>
<pre><code class="lang-javascript">{ type: &#39;ForStatement&#39;,

  //
  // This is `var i = 0;`
  //
  init:
   { type: &#39;VariableDeclaration&#39;,
     declarations:
      [ { type: &#39;VariableDeclarator&#39;,
          id: { type: &#39;Identifier&#39;, name: &#39;i&#39; },
          init: { type: &#39;Literal&#39;, value: 0, raw: &#39;0&#39; } },
        { type: &#39;VariableDeclarator&#39;,
          id: { type: &#39;Identifier&#39;, name: &#39;acc&#39; },
          init: { type: &#39;Literal&#39;, value: 0, raw: &#39;0&#39; } }],
     kind: &#39;var&#39; },

  //
  // `i &lt; arr.length`
  //
  test:
   { type: &#39;BinaryExpression&#39;,
     operator: &#39;&lt;&#39;,
     left: { type: &#39;Identifier&#39;, name: &#39;i&#39; },
     right:
      { type: &#39;MemberExpression&#39;,
        computed: false,
        object: { type: &#39;Identifier&#39;, name: &#39;arr&#39; },
        property: { type: &#39;Identifier&#39;, name: &#39;length&#39; } } },

  //
  // `i++`
  //
  update:
   { type: &#39;UpdateExpression&#39;,
     operator: &#39;++&#39;,
     argument: { type: &#39;Identifier&#39;, name: &#39;i&#39; },
     prefix: false },

  //
  // `arr[i] += 1;`
  //
  body:
   { type: &#39;ExpressionStatement&#39;,
     expression:
      { type: &#39;AssignmentExpression&#39;,
        operator: &#39;+=&#39;,
        left: { type: &#39;Identifier&#39;, name: &#39;acc&#39; },
        right:
         { type: &#39;MemberExpression&#39;,
           computed: true,
           object: { type: &#39;Identifier&#39;, name: &#39;arr&#39; },
           property: { type: &#39;Identifier&#39;, name: &#39;i&#39; } } } }
</code></pre>
<p>This JSON could also be visualized:
<img src="/images/ast.svg" alt="AST"></p>
<p>This is a tree, so it is very natural to traverse it from the top to the bottom,
generating the machine code as we visit the AST nodes. The problem with this
approach is that the information about variables is very sparse, and is spread
through the different tree nodes.</p>
<p>Again, to safely move the length lookup out of the loop we need to know that the
array length does not change between the loop&#39;s iterations. Humans can do it
easily just by looking at the source code, but the compiler needs to do quite a
lot of work to confidently extract those facts directly from the AST.</p>
<p>Like many other compiler problems, this is often solved by lifting the data into
a more appropriate abstraction layer, i.e. intermediate representation. In this
particular case that choice of IR is known as a data-flow graph (DFG). Instead
of talking about syntax-entities (like <code>for loop</code>s, <code>expressions</code>, ...), we
should talk about the data itself (read, variables values), and how it changes
through the program.</p>
<h2 id="data-flow-graph">Data-flow Graph</h2>
<p>In our particular example, the data we are interested in is the value of
variable <code>arr</code>. We want to be able to easily observe all uses of it to verify
that there are no out-of-bounds accesses or any other change that would modify
the length of the array.</p>
<p>This is accomplished by introducing &quot;def-use&quot; (definition and uses) relationship
between the different data values. Concretely, it means that the value has been
declared once (<em>node</em>), and that it has been used somewhere to create new values
(<em>edge</em> for every use). Obviously, connecting different values together will
form a <strong>data-flow graph</strong> like this:</p>
<p><img src="/images/data-flow.svg" alt="Data-flow Graph"></p>
<p>Note the red <code>array</code> box in this vast graph. The solid arrows going out of it
represent uses of this value. By iterating over those edges, the compiler can
derive that the value of <code>array</code> is used at:</p>
<ul>
<li><code>loadArrayLength</code></li>
<li><code>checkIndex</code></li>
<li><code>load</code></li>
</ul>
<p>Such graphs are constructed in the way that explicitly &quot;clones&quot; the array node,
if its value was accessed in a destructive manner (i.e. stores, length sizes).
Whenever we see <code>array</code> node and observe its uses - we are always certain that
its value does not change.</p>
<p>It may sound complicated, but this property of the graph is quite easy to
achieve. The graph should follow <a href="https://en.wikipedia.org/wiki/Static_single_assignment_form">Single Static Assignment</a> (SSA) rules.
In short, to convert any program to <a href="https://en.wikipedia.org/wiki/Static_single_assignment_form">SSA</a> the compiler needs to rename all
assignments and later uses of the variables, to make sure that each variable is
assigned only once.</p>
<p>Example, before SSA:</p>
<pre><code class="lang-javascript">var a = 1;
console.log(a);
a = 2;
console.log(a);
</code></pre>
<p>After SSA:</p>
<pre><code class="lang-javascript">var a0 = 1;
console.log(a0);
var a1 = 2;
console.log(a1);
</code></pre>
<p>This way, we can be sure that when we are talking about <code>a0</code> - we are actually
talking about a single assignment to it. This is really close to how people do
things in the functional languages!</p>
<p>Seeing that <code>loadArrayLength</code> has no control dependency (i.e. no dashed lines;
we will talk about them in a bit), compiler may conclude that this node is free
to move anywhere it wants to be and can be placed outside of the loop.
By going through the graph further, we may observe that the value of <code>ssa:phi</code>
node is always between <code>0</code> and <code>arr.length</code>, so the <code>checkIndex</code> may be removed
altogether.</p>
<p>Pretty neat, isn&#39;t it?</p>
<h2 id="control-flow-graph">Control Flow Graph</h2>
<p>We just used some form of <a href="https://en.wikipedia.org/wiki/Data-flow_analysis">data-flow analysis</a> to extract information from
the program. This allows us to make safe assumptions about how it could be
optimized.</p>
<p>This <em>data-flow representation</em> is very useful in many other cases too. The only
problem is that by turning our code into this kind of graph, we made a step
backwards in our representation chain (from the source code to the machine
code). This intermediate representation is less suitable for generating machine
code than even the AST.</p>
<p>The reason is that the machine is just a sequential list of instructions, which
the CPU executes one-after-another. Our resulting graph doesn&#39;t appear to
convey that. In fact, there is no enforced ordering in it at all.</p>
<p>Usually, this is solved by grouping the graph nodes into blocks. This
representation is known as a <a href="https://en.wikipedia.org/wiki/Control_flow_graph">Control Flow Graph</a> (CFG). Example:</p>
<pre><code>b0 {
  i0 = literal 0
  i1 = literal 0

  i3 = array
  i4 = jump ^b0
}
b0 -&gt; b1

b1 {
  i5 = ssa:phi ^b1 i0, i12
  i6 = ssa:phi ^i5, i1, i14

  i7 = loadArrayLength i3
  i8 = cmp &quot;&lt;&quot;, i6, i7
  i9 = if ^i6, i8
}
b1 -&gt; b2, b3
b2 {
  i10 = checkIndex ^b2, i3, i6
  i11 = load ^i10, i3, i6
  i12 = add i5, i11
  i13 = literal 1
  i14 = add i6, i13
  i15 = jump ^b2
}
b2 -&gt; b1

b3 {
  i16 = exit ^b3
}
</code></pre><p>It is called a graph not without the reason. For example, the <code>bXX</code> blocks
represent nodes, and the <code>bXX -&gt; bYY</code> arrows represent edges. Let&#39;s visualize
it:</p>
<p><img src="/images/cfg.svg" alt="CFG"></p>
<p>As you can see, there is code before the loop in block <code>b0</code>, loop header in
<code>b1</code>, loop test in <code>b2</code>, loop body in <code>b3</code>, and exit node in <code>b4</code>.</p>
<p>Translation to machine code is very easy from this form. We just replace <code>iXX</code>
identifiers with CPU register names (in some sense, CPU registers are sort of
variables, the CPU has a limited amount of registers, so we need to be careful
to not run out of them), and generating machine code for each instruction,
line-by-line.</p>
<p>To recap, <a href="https://en.wikipedia.org/wiki/Control_flow_graph">CFG</a> has data-flow relations and also ordering. This allows us to
utilize it for both data-flow analysis and machine code generation. However,
attempting to optimize the CFG, by manipulating the blocks and their contents
contained within it, can quickly become complex and error-prone.</p>
<p>Instead, Clifford Click and Keith D. Cooper proposed to use an approach
called <a href="http://www.researchgate.net/profile/Cliff_Click/publication/2394127_Combining_Analyses_Combining_Optimizations/links/0a85e537233956f6dd000000.pdf"><strong>sea-of-nodes</strong></a>, the very topic of this blog post!</p>
<h2 id="sea-of-nodes">Sea-of-Nodes</h2>
<p>Remember our fancy data-flow graph with dashed lines? Those dashed-lines are
actually what make that graph a <strong>sea-of-nodes</strong> graph.</p>
<p>Instead of grouping nodes in blocks and ordering them, we choose to declare the
control dependencies as the dashed edges in a graph. If we will take that graph,
remove everything <strong>non-dashed</strong>, and group things a bit we will get:</p>
<p><img src="/images/control-flow-sea.svg" alt="Control-flow part of Sea-of-Nodes"></p>
<p>With a bit of imagination and node reordering, we can see that this graph is the
same as the simplified CFG graphs that we have just seen above:</p>
<p><img src="/images/cfg.svg" alt="CFG"></p>
<p>Let&#39;s take another look at the <strong>sea-of-nodes</strong> representation:</p>
<p><img src="/images/data-flow.svg" alt="Sea-of-Nodes"></p>
<p>The striking difference between this graph and CFG is that there is no ordering
of the nodes, except the ones that have control dependencies (in other words,
the nodes participating in the control flow).</p>
<p>This representation is very powerful way to look at the code. It has all
insights of the general data-flow graph, and could be changed easily without
constantly removing/replacing nodes in the blocks.</p>
<h2 id="reductions">Reductions</h2>
<p>Speaking of changes, let&#39;s discuss the way to modify the graph. The sea-of-nodes
graph is usually modified by doing graph reductions. We just queue all nodes in
the graph. Invoke our reduction function for every node in the queue. Everything
that this function touches (changes, replaces) is queued back, and will be
passed to the function later on. If you have many reductions, you can just stack
them up together and invoke all of them on each node in the queue, or
alternatively, you can just apply them one after another, if they depend on the
final state of each other. It works like a charm!</p>
<p>I have written a JavaScript toolset for my sea-of-nodes experiments, which
includes:</p>
<ul>
<li><a href="https://github.com/indutny/json-pipeline">json-pipeline</a> - the builder and stdlib of the graph. Provides methods
to create nodes, add inputs to them, change their control dependencies, and
export/import the graph to/from the printable data!</li>
<li><a href="https://github.com/indutny/json-pipeline-reducer">json-pipeline-reducer</a> - the reductions engine. Just create a reducer
instance, feed it several reduction functions, and execute the reducer on the
existing <a href="https://github.com/indutny/json-pipeline">json-pipeline</a> graph.</li>
<li><a href="https://github.com/indutny/json-pipeline-scheduler">json-pipeline-scheduler</a> - library for putting back unordered graph in a
limited amount of blocks connected to each other by control edges (dashed
lines).</li>
</ul>
<p>Combined together, these tools can solve many problems that could be formulated
in terms of data-flow equations.</p>
<p>Example of reduction, which will optimize our initial JS code:</p>
<pre><code class="lang-javascript">for (var i = 0, acc = 0; i &lt; arr.length; i++)
  acc += arr[i];
</code></pre>
<h3 id="tl-dr">TL;DR</h3>
<p>This code chunk is quite big, so if you want to skip it - here are the notes of
what we will do below:</p>
<ul>
<li>Compute integer ranges of various nodes: literal, add, phi</li>
<li>Compute limits that apply to branch&#39;s body</li>
<li>Apply range and limit information (<code>i</code> is always a non-negative number limited
by <code>arr.length</code>) to conclude that length check is not necessary and can be
removed</li>
<li><code>arr.length</code> will be moved out of the loop automatically by
<code>json-pipeline-scheduler</code>. This is because it does <a href="https://courses.cs.washington.edu/courses/cse501/04wi/papers/click-pldi95.pdf">Global Code Motion</a> to
schedule nodes in blocks.</li>
</ul>
<script src="https://gist.github.com/indutny/781757293040e8a38efb.js"></script>

<p>Thank you for reading this. Please expect more information about this
sea-of-nodes approach.</p>
<hr>
<p>Special thanks to <a href="https://github.com/paulfryzel">Paul Fryzel</a> for proof-reading this, and providing
valuable feedback and grammar fixes!</p>
]]></description><link>http://darksi.de/d.sea-of-nodes</link><guid isPermaLink="true">http://darksi.de/d.sea-of-nodes</guid><dc:creator><![CDATA[Fedor Indutny]]></dc:creator><pubDate>Thu, 08 Oct 2015 00:00:00 GMT</pubDate></item><item><title><![CDATA[Diving into C++ internals of node]]></title><description><![CDATA[<h2 id="intro">Intro</h2>
<p>There is nothing to be scared about in the C++ internals of the project,
especially in internals of <a href="https://github.com/nodejs/io.js">io.js</a> and <a href="https://github.com/nodejs/node">node.js</a>.</p>
<p>If you ever tried to optimize JavaScript code to squeeze out every possible
performance or memory usage improvement out of it - you already wrote some C++
code.</p>
<p>Many blogs, workshops mention JavaScript optimizations, and some of the popular
suggestions are:</p>
<h3 id="hidden-classes">Hidden Classes</h3>
<p>Declare all properties in the constructor to avoid creating extra
&quot;hidden classes&quot;. This makes them pretty much the same as a C structures,
or C++ classes, where properties are declared ahead of time to help the
compiler optimize access to them.</p>
<p>Example:</p>
<pre><code class="lang-javascript">function Point(x, y, z) {
  this.x = x
  this.y = y
  this.z = z
}
</code></pre>
<p>Similar code in C++:</p>
<pre><code class="lang-c">class Point {
 public:
  double x;
  double y;
  double z;
};
</code></pre>
<h3 id="avoid-polymorphism">Avoid Polymorphism</h3>
<p>Avoid storing different types of values in a variables, and avoid passing
different types of values as an arguments to the function. This principle could
also be called &quot;Make your code monomorphic&quot;, or &quot;don&#39;t mess with Compiler&quot;.
This makes code look like as it has static typing, which is what we do in
C++.</p>
<pre><code class="lang-javascript">function add(x, y) {
  return x + y;
}

add(0, 1); // &lt;- good
add(&#39;foo&#39;, &#39;bar&#39;); // &lt;- polymorphism!
</code></pre>
<p>Compare to:</p>
<pre><code class="lang-c">int add(int x, int y) {
  return x + y;
}
</code></pre>
<h3 id="cache-and-reuse">Cache and Reuse</h3>
<p>Cache and reuse instances of objects that are expensive to create and are
allocated often. This is one is similar to manual memory allocation in C++.</p>
<pre><code class="lang-javascript">function Parser() {
}

Parser.freelist = [];

Parser.get = function() {
  if (this.freelist.length)
    return this.freelist.pop();
  return new Parser();
};

Parser.prototype.release = ...;
</code></pre>
<p>In C++:</p>
<pre><code class="lang-c">Parser* p = new Parser();
delete p;
</code></pre>
<p>To conclude, even if you never wrote C++ code, you actually very likely did it
in JS.</p>
<p>It is no surprise we use C++ in io.js/node.js. After all, V8 is written in C++
and it provides only a limited set of ECMAScript JavaScript APIs. They are
definitely cool, but if you got used to <code>setTimeout()</code> / <code>clearTimeout()</code> -
you&#39;ll be pretty disappointed to use just plain ECMA.</p>
<p>Our C++ layer lives on top of the event-loop and provides all sorts of APIs:
from net sockets to dns queries, from file system events to the zlib bindings.
Which is the main reason why node.js was created in the first place!</p>
<h2 id="short-history-of-c-layer">Short History of C++ layer</h2>
<p><img src="/images/history_of_git_blame.jpg" alt="History of git blame"></p>
<p>To better understand all of these, and to ease the contribution process - it
might be a good idea to start with the history of the subject. Luckily, from its
inception, node.js is using VCS, in particular git, so the history of the
development might be revealed by running <code>git log</code> and <code>git blame</code> on it.</p>
<p>Briefly, <code>git log deps/v8</code> - has the history of v8 fighting us, and
<code>git log src/</code> - has the history of us fighting v8.</p>
<h2 id="very-first-version">Very first version</h2>
<p>Jokes aside, everything started from <a href="https://github.com/nodejs/io.js/commit/61890720">61890720</a> commit. The commit log
just says:</p>
<pre><code>add readme and initial code
</code></pre><p>Unfortunately, we can&#39;t elaborate much from it, and need to figure out the
details ourselves. What do we see there?</p>
<ul>
<li><a href="https://github.com/taf2/libebb">libebb</a> - which was used as an HTTP parser. Ryan used the code
from the <a href="https://github.com/gnosek/ebb">Ebb</a> server that he has previously written for Ruby</li>
<li><a href="https://cs.fit.edu/code/projects/cse2410_fall2014_bounce/repository/revisions/90fc8d36220c0d66c352ee5f72080b8592d310d5/show/deps/liboi">liboi</a>- which was as a TCP server framework on top of the <a href="http://software.schmorp.de/pkg/libev.html">libev</a>.
liboi stands for <code>Library for Output Input</code></li>
</ul>
<p>So the first code (that actually started compiling only at <a href="https://github.com/nodejs/io.js/commit/7b7ceea">7b7ceea</a>) only
had one HTTP server and supplied JavaScript source code was just a handler for
it.</p>
<pre><code class="lang-javacsript">function Process(request) {
  if (options.verbose) {
    log(&quot;Processing &quot; + request.host +
        request.path +
        &quot; from &quot; + request.referrer +
        &quot;@&quot; + request.userAgent);
  }
  if (!output[request.host])
    output[request.host] = 1;
  else
    output[request.host]++
}
</code></pre>
<p>How was it organized internally?</p>
<p>There was a <code>server.cc</code> file which was reading the command line options, loading
the JavaScript source file, feeding all of these into V8, and starting the HTTP
server.</p>
<p>Second C++ file was <code>js_http_request_processor.cc</code> and it was responsible for
invoking the JavaScript http request handler. Not that much for a separate C++
file, right?</p>
<p>It wasn&#39;t working that much at that point, and didn&#39;t have any of
functionality that is provided today. So let&#39;s conclude and move on from it
quickly.</p>
<p>This version is characterized by following:</p>
<ul>
<li>One file to setup V8 and let JavaScript know about command-line arguments</li>
<li>HTTP server fully implemented in C/C++, not invoking the JavaScript for any
networking activities</li>
<li>One C++ instance per every incoming request, this instance maps some of the
HTTP fields (like host, url, method) to the JavaScript object.</li>
</ul>
<p>The last bullet point is very important to note: the C++ instance <-> JS object
mapping is a building brick of all future releases of node.js (including the
present one).</p>
<h2 id="064c8f02">064c8f02</h2>
<p>Now we quickly jump to <a href="https://github.com/nodejs/io.js/commit/064c8f02">064c8f02</a>. The commit log says:</p>
<pre><code>Use ObjectWrap base class for File, Socket, Server.
</code></pre><p>And this is the point where node.js has introduced one API to wrap all objects.</p>
<p><code>net.Server</code>, <code>net.Socket</code>, and <code>File</code> C++ classes are children of this
<code>ObjectWrap</code> class. Which means that for every instance of them -
there will be one instance of a JS object. Invoking methods on this JS object
will invoke C++ methods on the corresponding C++ class, and the constructor
itself is a C++ class constructor.</p>
<p>There are now different files for different parts of the provided API:</p>
<ul>
<li><code>src/node.cc</code> to set up C++ libraries and invoke <code>src/main.js</code> which
loads the script file and does some JavaScript initialization. (At this commit
we started to write as much code as possible in JavaScript, and leave
the rest in the C++ land. This pattern is used in io.js and node.js now too)</li>
<li><code>src/http.cc</code> - http server API, Connection, HttpRequest objects</li>
<li><code>src/file.cc</code>, <code>src/file.js</code> - future <code>fs</code> module.
<code>src/file.js</code> consists of the API abstractions for the C++ layer,
basically the same thing as with <code>src/node.cc</code> and <code>src/main.js</code></li>
<li><code>src/process.cc</code> has only <code>exit()</code> method so far, will evolve into the
<code>process</code> object</li>
<li><code>src/timers.cc</code> is about <code>setTimeout</code>/<code>setInterval</code></li>
</ul>
<p>Just a side note: HTTP server is still provided by <a href="https://cs.fit.edu/code/projects/cse2410_fall2014_bounce/repository/revisions/90fc8d36220c0d66c352ee5f72080b8592d310d5/show/deps/liboi">liboi</a>, and node.js is
using <a href="http://software.schmorp.de/pkg/libev.html">libev</a>.</p>
<h2 id="v0-2">v0.2</h2>
<p>There was lots of growing and maturing from that commit to the v0.2, and most
notable of them were about separating the JS parts from the C++ ones,
adding CommonJS support, and tons of new modules! The file structure is
beginning to look like what we have now:</p>
<ul>
<li><code>lib/</code> folder for all JavaScript CommonJS modules</li>
<li><code>src/</code> for their C++ counterparts</li>
<li><code>deps/</code> for all dependencies: v8, http-parser, c-ares (for async DNS),
libeio (for async FS), and libev (for async networking and auxiliary stuff)</li>
</ul>
<p>Previously barely used through the <code>src/</code>, <code>ObjectWrap</code> now became a public API,
which helped polish it out a lot and improved our core use case as well.</p>
<p>Very importantly, in <a href="https://github.com/nodejs/io.js/commit/064c8f02">064c8f02</a> all C++ interfaces were global objects. In
v0.2 they are provided by <code>process.binding</code> and are thus not directly visible to
the user&#39;s code.</p>
<p>For example, <code>process.binding(&#39;fs&#39;)</code>:</p>
<pre><code class="lang-javascript">&gt; process.binding(&#39;fs&#39;);
{ access: [Function: access],
  close: [Function: close],
  open: [Function: open],
  ...lots of stuff...
</code></pre>
<p>returns lots of C++ methods and classes that are heavily used for interoperation
between C++ and JS in <code>lib/fs.js</code>. Similar stuff is done for the rest of the
<code>lib/</code> modules.</p>
<h2 id="v0-6">v0.6</h2>
<p>Just a short note: <code>libev</code> was removed and replaced by <a href="https://github.com/libuv/libuv">libuv</a>. A product of
lots of work by Ben Noordhuis, Bert Belder, Ryan Dahl, and others!</p>
<p>The v0.6 version is a major milestone in evolution of node.js. Partly because
Windows is now in the list of the officially supported platforms, partly because
we have our own single event-loop platform that supports both async File System
and async networking operations.</p>
<h2 id="v0-10">v0.10</h2>
<p>Good, stable, but boring...</p>
<h2 id="v0-12-and-io-js">v0.12 and io.js</h2>
<p>Lots of new stuff! :)</p>
<p>Mainly, we have outgrown the <code>ObjectWrap</code> to accommodate the tracing API (which
is still needs lots of rework, AFAIK). The hip thing now is <code>AsyncWrap</code> which is
in many ways the same thing, but now is attached to some particular domain of
operation (i.e. http, dns, tls) and which might have the another <code>AsyncWrap</code> as
a parent. Note that <code>ObjectWrap</code> lives in <code>src/node_object_wrap.h</code>, and
<code>AsyncWrap</code> in <code>src/async-wrap.h</code>.</p>
<p>This is now the present point of the node.js evolution, and I would like to
stop with the Software Archeology at this point.</p>
<h2 id="interoperation-handles-wraps-and-unicorns-">Interoperation, handles, wraps, and unicorns!</h2>
<p>We are finally ready to dive into the C++ internals, and explore them in a
greater detail.</p>
<p>As we already figured out - whole APIs provided by the node.js/io.js live in
two folders: <code>lib</code> and <code>src</code>. <code>lib</code> holds the core modules, <code>src</code> holds their
C++ counterparts.</p>
<p>When you call <code>require(&#39;fs&#39;)</code> - it does nothing but just executes the contents
of the <code>lib/fs.js</code> file. No magic here.</p>
<p>Now comes the interesting part, JavaScript is not capable of file system
operations, nor it is capable of networking. This is actually for the
best! (You don&#39;t want your browser to mess up whole file system,
right?) So when you do <code>fs.writeFileSync</code>, or when you are calling
<code>http.request()</code> there is a lot of low-level C++ stuff happening outside of the
JS-land.</p>
<p>While the <code>fs</code> module is quite simple to explain, it is quite boring too. After
all, in most of the cases it is just using number to represent the opened
file (so called <code>file descriptor</code>), and it is passing this number around:
from C++ to JS, and from JS to C++. Nothing interesting, let&#39;s move on!</p>
<p>Certainly much more attractive is the <code>net</code> module. We create sockets, get
the <code>connect</code> events, and expect the <code>.write()</code> callbacks to be eventually
invoked. All of these should be powered by the C++ machinery!</p>
<p>Here is where most of the interoperation is happening. The
<code>tcp_wrap</code> and <code>stream_wrap</code> bindings (remember, <code>process.binding()</code>, right?)
provide very useful classes for JS-land: TCP, TCPConnectWrap, WriteWrap,
ShutdownWrap.</p>
<ul>
<li><code>TCP</code> holds the TCP socket and provides methods for writing and reading
stuff</li>
<li><code>*Wrap</code> objects are what you pass to the <code>TCP</code> methods when you expect
some async action to happen, and need to receive notification (callback) on
their completion.</li>
</ul>
<p>For example, the normal workflow for <code>net.connect()</code> follows:</p>
<ul>
<li>Create <code>TCP</code> instance in <code>lib/net.js</code>, store it in the <code>_handle</code> property of
the <code>net.Socket</code> object</li>
<li>Parse all arguments to <code>net.connect()</code></li>
<li>Create <code>TCPConnectWrap</code> instance (usually named <code>req</code>)</li>
<li>Invoke <code>.connect()</code> method with <code>req, port, host</code></li>
<li>Get <code>req.oncomplete</code> function invoked eventually, once the connection was
established, or once the kernel reported an error</li>
</ul>
<p>In conclusion: most of the C++ classes are either handles, or requests.
Requests are very temporary and never outlive the handle that they are bound to,
while the handles are something that live much longer (i.e. for the entire life
time of the TCP connection).</p>
<p>Speaking of the file structure: <code>TCP</code> is represented by the <code>TCPWrap</code> class in
<code>src/tcp_wrap.cc</code>, <code>TCPConnectWrap</code> lives in the same place, and <code>WriteWrap</code>
is in the <code>stream_base.cc</code> file (in io.js).</p>
<h2 id="structure-of-c-files">Structure of C++ files</h2>
<p>But how does the C++ provide this classes to JavaScript?</p>
<p>Each binding has a <code>NODE_MODULE_CONTEXT_AWARE_BUILTIN</code> macro that registers it
in the <code>node.cc</code>. This has the same effect as following JavaScript snippet:</p>
<pre><code class="lang-javascript">modules[moduleName] = {
  initialized: false,
  initFn: moduleInitFn
};
</code></pre>
<p>When <code>process.binding(&#39;moduleName&#39;)</code> is invoked, <code>node.cc</code> looks up the proper
internal binding in this hashmap and initializes it (if it wasn&#39;t previously
initialized) by calling the supplied function.</p>
<pre><code class="lang-javascript">process.binding = function(moduleName) {
  var module = modules[moduleName];
  if (module.initialized)
    return module.exports;

  module.exports = {};
  module.initFn(module.exports);
  return module.exports;
};
</code></pre>
<p>This initialization function receives <code>exports</code> object as an input, and exports
the methods and classes to it in pretty much the same way as you normally do
in CommonJS modules.</p>
<p>Each of the exported classes are bound to some C++ classes, and most of them are
actually derived from the <code>AsyncWrap</code> C++ class.</p>
<p>The Handle instances are destroyed automatically by V8&#39;s GC (once they are
closed in JS), and the Wraps are manually destroyed by the Handle, once they are
not used anymore.</p>
<p>Side-note:</p>
<p>there are two types of references to the JS
objects from C++ land: normal and weak. By default <code>AsyncWrap</code>s are referencing
their objects in a <code>normal</code> way, which means that the JS objects representing
the C++ classes won&#39;t be garbage collected until C++ class will dispose the
reference. The weak mode is turned on only when the <code>MakeWeak</code> is called
somewhere in C++. This might be very useful when debugging memory leaks.</p>
<h2 id="small-exam">Small exam</h2>
<h3 id="situation">Situation</h3>
<p>You debug some io.js/node.js issue, and find that it is crashing when
instantiating a class provided by <code>process.binding(&#39;broken&#39;)</code>. Where will you
attempt to search for the C++ source code of that class?</p>
<h3 id="answer">Answer</h3>
<p>Somewhere in <code>src/</code>. Find
<code>NODE_MODULE_CONTEXT_AWARE_BUILTIN(broken, ...)</code> and it is most like going to be
in <code>src/broken_something.cc</code>.</p>
<h2 id="c-streams">C++ Streams</h2>
<p>Now comes one of my recent obsessions. The C++ Stream API.</p>
<p>It is a established fact for me that exposing the building blocks of APIs helps
to renovate, reshape and make them better <em>a lot better</em>. One of such thing
that I was always keen to re-do was a <code>StreamWrap</code> instance.</p>
<p>It was ok-ish in v0.10, but when we moved TLS (SSL) implementation into C++
land it changed dramatically... and, honestly saying, not in a good way.</p>
<p>The previously singular <code>StreamWrap</code> instance, now became a monster that was
capable of passing the incoming data elsewhere, skipping the JavaScript
callbacks completely and doing some dark-magic OpenSSL machinery on top of it.
The implementation worked like a charm, providing much better TLS performance,
but the source code became cluttered and rigid.</p>
<p>This &quot;move-parsing-to-elsewhere&quot; thing reminded me a lot about the
<code>stream.pipe</code> that we had for JavaScript streams for ages. The natural thing to
do about it was to introduce something similar in the C++ land too. This is
exactly what was done in io.js, and the results of this live in
<code>src/stream_base.cc</code>.</p>
<h2 id="next-step-with-the-c-stream-apis">Next step with the C++ Stream APIs</h2>
<p>Now we have a very general implementation of this thing that could be reused in
many places. The first thing that I expect will be using this might be an
HTTP2 stream. To do it in core, we should do it in user-land first, and it could
be accomplished only by exposing the C++ Stream API, in the same way as we did
it with ObjectWrap.</p>
<h2 id="epilogue">Epilogue</h2>
<p>I&#39;m going to ask you to:</p>
<ul>
<li>Clone the io.js repo</li>
<li>Open the <code>src/</code></li>
<li>Go through files in it, and check what you read about it</li>
<li>Open <code>src/stream_base.h</code>, <code>src/stream_base.cc</code> and friends and figure out
what seems to be wrong to you</li>
<li><a href="https://github.com/nodejs/io.js/pulls">Send a PR</a></li>
<li>Have fun!</li>
</ul>
]]></description><link>http://darksi.de/c.cpp-in-node</link><guid isPermaLink="true">http://darksi.de/c.cpp-in-node</guid><dc:creator><![CDATA[Fedor Indutny]]></dc:creator><pubDate>Sat, 16 May 2015 00:00:00 GMT</pubDate></item><item><title><![CDATA[Side Projects]]></title><description><![CDATA[<p>After reading <a href="https://github.com/antirez">antirez</a>&#39;s <a href="http://antirez.com/news/86">blog post</a> I decided that it might be a good
exercise to write down the notable side projects that I spent my time upon since
Jan 2014.</p>
<p>Here is the list and some comments from me:</p>
<h3 id="bn-js"><a href="https://github.com/indutny/bn.js">bn.js</a></h3>
<p>JavaScript library for working with Big Numbers. <a href="https://github.com/indutny/bn.js">bn.js</a> is an ultra-fast
<a href="https://github.com/justmoon/node-bignum">bignum</a> alternative with support for running in io.js/node.js and browsers.</p>
<p>This one took lots of time and effort through whole year with some periodic
<a href="https://github.com/indutny/bn.js/graphs/contributors">sparks in a contributions graph</a>, and many PRs from OpenSource community.
Seriously, big kudos to you people for helping me with it!</p>
<h3 id="elliptic"><a href="https://github.com/indutny/elliptic">elliptic</a></h3>
<p>JS library for doing Elliptic Curve crypto. It was the reason for creating the
<a href="https://github.com/indutny/bn.js">bn.js</a> in the first place, and excuse for me to learn more about EC
cryptography and crazy math behind it.</p>
<h3 id="bud"><a href="https://github.com/indutny/bud">bud</a></h3>
<p>A friendly and clever TLS-terminating proxy in C.</p>
<p>Although I worked on it since Nov 2013, lots of development happened during the
2014 year. This is my biggest project in C so far, and it has taught me a lot
about designing the APIs and interfaces in low-level languages.</p>
<p>Bud seens lots of love from <a href="https://github.com/odeke-em">Emmanuel Odeke</a>. Big thanks to you, Emmanuel!</p>
<h3 id="tls-js"><a href="https://github.com/indutny/tls.js">tls.js</a></h3>
<p>(Incomplete) TLS implementation in JavaScript.</p>
<p>Totally experimental protocol implementation. Did it just for fun, but it turned
to be useful in screening the web servers.</p>
<h3 id="hash-js"><a href="https://github.com/indutny/hash.js">hash.js</a></h3>
<p>Implementations of SHA1, SHA224, SHA256, SHA384, SHA512, RIPEMD160, various
HMACs. One of the mandatory dependencies of...</p>
<h3 id="bcoin"><a href="https://github.com/indutny/bcoin">bcoin</a></h3>
<p>BitCoin SPV client implementation. Purely experimental, but I heard that some
people do use it.</p>
<p>Lots of contributions from <a href="https://github.com/chjj">Christopher Jeffrey</a> here. Thank you!</p>
<h3 id="bthread-src-"><a href="https://chrome.google.com/webstore/detail/bthread/ldbfhhncehnfgppdlgjhfgffachpehkd">bthread</a> (<a href="https://github.com/indutny/bthread">src</a>)</h3>
<p>Writing blog posts in a BitCoin block-chain.</p>
<p><em>I know many people hate me for this, but still I wanted to experiment with it
a little</em>.</p>
<h3 id="js-js"><a href="https://github.com/js-js/js.js">js.js</a></h3>
<p>JS JIT compiler written in JS.</p>
<p>Very preliminary implementation with little or no compatibility with ECMAScript
yet :)</p>
<h3 id="heap-js-jit-js-cfg-js-"><a href="https://github.com/js-js">heap.js, jit.js, cfg.js, ...</a></h3>
<p>Various <a href="https://github.com/js-js/js.js">js.js</a> dependencies.</p>
<h3 id="lll-reduction"><a href="https://github.com/indutny/lll-reduction">lll-reduction</a></h3>
<p><a href="http://en.wikipedia.org/wiki/Lenstra%E2%80%93Lenstra%E2%80%93Lov%C3%A1sz_lattice_basis_reduction_algorithm">LenstraLenstraLovsz algorithm</a> JavaScript implementation.</p>
<p>I don&#39;t remember exact reasons for writing this, but I guess I thought about
doing more optimal <a href="http://www.hyperelliptic.org/tanja/conf/ECC08/slides/Mike-Scott.pdf">GLV Method</a> for <a href="https://github.com/indutny/elliptic">elliptic</a>.</p>
<h3 id="core2dump"><a href="https://github.com/indutny/core2dump">core2dump</a></h3>
<p>Creating heap snapshots out of the core file on OS X, linuxes, and FreeBSD.</p>
<h3 id="asn1-js"><a href="https://github.com/indutny/asn1.js">asn1.js</a></h3>
<p>ASN.1 encoding implementation in JS.</p>
<h3 id="miller-rabin"><a href="https://github.com/indutny/miller-rabin">miller-rabin</a></h3>
<p>Miller-Rabin primality test in JS.</p>
<h3 id="caine"><a href="https://github.com/indutny/caine">caine</a></h3>
<p>Butler bot for github projects. I wanted it to be used for io.js, but we decided
to walk a different road.</p>
<h4 id="epilogue">Epilogue</h4>
<p>I guess that&#39;s it.</p>
<p>Most of these projects were a big incentive for me to dig into the protocols,
technology, science. I find it much more enjoyable and interesting to
investigate new topics through their applications.</p>
<p><strong>Thank you for all your contributions, people! It is really awesome to see
you interested in helping me with these and other projects!</strong></p>
<p>Thanks for reading this.</p>
]]></description><link>http://darksi.de/b.side-projects</link><guid isPermaLink="true">http://darksi.de/b.side-projects</guid><dc:creator><![CDATA[Fedor Indutny]]></dc:creator><pubDate>Thu, 26 Feb 2015 00:00:00 GMT</pubDate></item><item><title><![CDATA[Deoptimize me not, v8]]></title><description><![CDATA[<p>Compilers are awesome, right? If any programming concept may exist, it will
probably be used in compiler implementation at some point. I am always amazed
by my findings during v8 bug triaging or just random code exploration.</p>
<p>The interesting thing about v8 that I was always passionate about, but never
truly understood, was the Deoptimizer. The idea here is that v8 optimizes
code to make it run faster, but this optimization relies on assumptions
about types, ranges, actual values, const-ness, etc. These assumptions imply
that the optimized code won&#39;t run when these conditions are not met,
since the compiler needs to &quot;deoptimize&quot; it by returning to the previous
&quot;no-assumptions&quot; version of generated code when the assumptions are failing.</p>
<p>Technically it means that the compiler is in fact two compilers:
a base compiler and an &quot;optimizer&quot;. (Or even more, if we are talking about JSC
and SpiderMonkey). The concept is quite sound and can yield incredible
performance, but there is a nuance: the optimized code may be &quot;deoptimized&quot; in
various places, not just at the entry point, meaning that the environment (local
variables, arguments, context) should be mapped and moved around.</p>
<h2 id="stack-machines">Stack machines</h2>
<p>To better understand what needs to be done and how things are happening let&#39;s
consider a basic stack machine, like the one that we might use to interpret
program instead of JIT compiling it.</p>
<p><em>Note that this stack machine and assembly below are just an output of some
abstract compiler and has nothing do to with v8. Thus here only for
demonstration purposes</em></p>
<pre><code>push a
push b
push c
mul     ; pop 2 values and push `arg0 * arg1`
push d
mul     ; b * c * d
add     ; pop 2 values and push `arg0 + arg`
ret     ; pop and return value
</code></pre><p>The interpreter will execute instructions one-by-one, maintaining the stack at
every point.</p>
<p>Now we let&#39;s imagine some register machine (like x86_64), and write down
the same program in assembly language. To make it a bit more interesting,
consider that the target architecture has only two registers and the rest of the
values need to be stored in memory (on-stack).</p>
<pre><code class="lang-asm">mov [slot0], a   ; store value in 0 memory slot
mov rax, b       ; store value in rax register
mov rbx, c       ; store value in rbx register
mul rax, rbx     ; rax = rax * rbx
mov rbx, d
mul rax, rbx     ; rax = b * c * d
mov rbx, [slot0] ; load value from 0 memory slot
add rax, rbx     ; rax = b * c * d + a
</code></pre>
<p>The instructions are executed one-by-one, maintaining the register values and
memory slots.</p>
<p>In terms of our compiler, the former code is an unoptimized version of our
program, and the latter one is optimized. In fact, this is a completely valid
claim if we would like to run it on x86_64 platform, as assembly has much higher
execution speed than interpreted code that needs to be emulated.</p>
<p>Suppose that the second <code>mul</code> instruction in assembly works only when the <code>d</code>
(which is in <code>rbx</code> register) is a small integer. Now if the execution will
reach the <code>mul</code> and find that there is a JavaScript string, it will just fail
to do the &quot;right thing&quot;. This <code>mul(num, str)</code> operation will definitely require
some sort of type coercion, and could be easily handled by the interpreter.
Doing it in assembly will very likely be much more costly in terms of
performance. To deal with this the compiler inserts check instructions:</p>
<pre><code>mov [slot0], a
mov rax, b
mov rbx, c
checkSmallInt rax
checkSmallInt rbx
mul rax, rbx
mov rbx, d
checkSmallInt rax
checkSmallInt rbx
mul rax, rbx ;
mov rbx, [slot0]
add rax, rbx
</code></pre><p>So in such an uncommon case, where the argument of <code>mul</code> is not a small integer,
this code should somehow be &quot;deoptimized&quot; from assembly code to the stack
machine and continue execution in the interpreted version. Here is the position
in the optimized code where it will stop:</p>
<pre><code>mov [slot0], a
mov rax, b
mov rbx, c
checkSmallInt rax
checkSmallInt rbx
mul rax, rbx
mov rbx, d
checkSmallInt rax
checkSmallInt rbx &lt;-----
mul rax, rbx ;
mov rbx, [slot0]
add rax, rbx
</code></pre><p>...and position in unoptimized code, where would like it to continue:</p>
<pre><code>push a
push b
push c
mul
push d
mul     ; &lt;-----
add
ret
</code></pre><p>How could it do that? The simplest way is just to re-execute all code from the
program&#39;s entry point using the input arguments. This solution is very limited
though, because it is possible only if the optimized function was pure, or in
other words had no instructions with side effects (like function calls, etc...).</p>
<p>The more general solution is to find all live values (the ones that may be used
by later functions) at the deoptimization point, find their locations in both
optimized and unoptimized code, and copy the values from the registers/memory
to stack machine&#39;s slot.</p>
<p>This is exactly what the &quot;deoptimizer&quot; in v8 does. The main difference from our
imaginary example is that both unoptimized and optimized codes are in <code>x86_64</code>
assembly language.</p>
<h2 id="simulates">Simulates</h2>
<p>Now we know what to do, but how is it actually implemented in v8?</p>
<p>These mappings are possible thanks to the special high-level instructions called
<code>Simulate</code>s. This is how they look in the v8&#39;s high-level intermediate
representation (abbr. IR, see my <a href="https://www.youtube.com/watch?v=tf6YTgO6Org">EmpireNode talk</a> for more info on the IRs):</p>
<pre><code>v9 BlockEntry  &lt;|@
v10 Simulate id=3 var[3] = t8 &lt;|@
v11 StackCheck  changes[NewSpacePromotion] &lt;|@
v12 UseConst t8 &lt;|@
t13 ThisFunction  &lt;|@
t14 CheckNonSmi t3 &lt;|@
t15 CheckMaps t3 [0x2e26d7019781] &lt;|@
v16 CheckPrototypeMaps [...] &lt;|@
v17 Simulate id=24 push t3, push t4, push t8, var[3] = t13 &lt;|@
v18 EnterInlined middle, id=4 &lt;|@
t54 PushArgument t3 &lt;|@
t55 PushArgument t4 &lt;|@
t56 ArgumentsElements  &lt;|@
v19 UseConst t1 &lt;|@
t20 Constant ... &lt;|@
v25 Simulate id=26 pop 1, push t19, var[3] = t2, var[4] = t20 &lt;|@
</code></pre><p>(Note that you can obtain such IR by running node.js with <code>--trace-hydrogen</code>
flag, which will print it out into the <code>hydrogen.cfg</code> or <code>hydrogen-&lt;pid&gt;.cfg</code>
file).</p>
<p>The thing is called <code>Simulate</code> with good reason. Strip away all other
instructions:</p>
<pre><code>v10 Simulate id=3 var[3] = t8 &lt;|@
v17 Simulate id=24 push t3, push t4, push t8, var[3] = t13 &lt;|@
v25 Simulate id=26 pop 1, push t19, var[3] = t2, var[4] = t20 &lt;|@
</code></pre><p>...and we will see something that resembles... our simplified stack machine!
Having a stack machine means that we could &quot;simulate&quot; it&#39;s state by executing
instructions one-by-one. v8&#39;s has a couple of them:</p>
<ul>
<li><code>var[index] = value</code> - put a value in some on-stack slot</li>
<li><code>push value</code> - push a value to a virtual stack</li>
<li><code>pop count</code> - pop <code>count</code> of values from the stack</li>
</ul>
<p>Let&#39;s simulate some states out of the above sample:</p>
<pre><code>v10: var = { 3: t8 }, stack = []
v17: var = { 3: t13 }, stack = [ t3, t4, t8 ]
v25: var = { 3: t13, 4: t20 }, stack = [ t3, t4, t19 ]
</code></pre><p><em>Note that this &quot;simulation&quot; happens at compile-time, not when actually
deoptimizing.</em></p>
<p>These states can be used to map the values from optimized to unoptimized code.
For example, if we would like to &quot;deoptimize&quot; at <code>t56</code>, we will have to find the
latest state which was at <code>v17</code>: <code>var = { 3: t13 }, stack = [ t3, t4, t8 ]</code>, and
just place the present values into a proper stack slots and local variables (for
<code>var</code> ones).</p>
<p>With the <code>--trace-deopt</code> flag v8 will give us some insights on how it is doing
this:</p>
<pre><code>**** DEOPT: outer at bailout #14, address 0x0, frame size 56
[deoptimizing: begin 0x341ad2082a49 outer @14]
translating outer =&gt; node=24, height=8
0x7fff5fbff3e8: [top + 72] &lt;- 0xb7720f7d8b9 ; [sp + 96] 0xb7720f7d8b9 &lt;an O&gt;
0x7fff5fbff3e0: [top + 64] &lt;- 0xb7720f7d8b9 ; rbx 0xb7720f7d8b9 &lt;an O&gt;
0x7fff5fbff3d8: [top + 56] &lt;- 0x21ba55263fa9 ; caller&#39;s pc
0x7fff5fbff3d0: [top + 48] &lt;- 0x7fff5fbff410 ; caller&#39;s fp
0x7fff5fbff3c8: [top + 40] &lt;- 0xb7720f7d479; context
0x7fff5fbff3c0: [top + 32] &lt;- 0x341ad2082ad9; function
0x7fff5fbff3b8: [top + 24] &lt;- 0x341ad2004121 &lt;undefined&gt; ; literal
0x7fff5fbff3b0: [top + 16] &lt;- 0x341ad2082ad9 &lt;JS Function inner&gt; ; literal
0x7fff5fbff3a8: [top + 8] &lt;- 0xb7720f7d8b9 ; [sp + 24] 0xb7720f7d8b9 &lt;an O&gt;
0x7fff5fbff3a0: [top + 0] &lt;- 0x341ad2004121 ; rax 0x341ad2004121 &lt;undefined&gt;
</code></pre><p>Arrows here indicate the direction of movement. The output frame of the
unoptimized code is on the left side, and on the right side - optimized code&#39;s
values.</p>
<p>The mentioned frame is an on-stack structure used for storing the caller address
(to make <code>return</code> statements work), caller&#39;s frame address, and sometimes some
additional stuff (like JS context, <code>this</code>, arguments, and the function itself):</p>
<p><img src="/images/callstack.png" alt="Callstack"></p>
<p>Ignoring all the internal frame things, the interesting part would be:</p>
<pre><code>translating outer =&gt; node=24, height=8
0x7fff5fbff3a8: [top + 8] &lt;- 0xb7720f7d8b9 ; [sp + 24] 0xb7720f7d8b9 &lt;an O&gt;
0x7fff5fbff3a0: [top + 0] &lt;- 0x341ad2004121 ; rax 0x341ad2004121 &lt;undefined&gt;
</code></pre><p>The high-level IR of the code that generated this trace contained:</p>
<pre><code>0 0 v10 Simulate id=3 var[3] = t8 &lt;|@
...
0 0 v17 Simulate id=24 push t3, push t4 &lt;|@
</code></pre><p>There is only one simulate instruction, and the state is: <code>stack = [t3, t4]</code>.
(Sorry ignoring the local variables for this blog post).
Thus, the deoptimizer needs to put the values of the <code>t3</code> and <code>t4</code> instructions
into the stack slots. This information was stored ahead of time, and will be
looked up right when deoptimizing the code. Here, <code>t3</code> was in the <code>[sp + 24]</code>
stack slot in the optimized code, and <code>t4</code> was in <code>rax</code>. This process is called
a &quot;frame translation&quot;. Afterwards the execution will be redirected to the
unoptimized code, which will just continue operating on the values at the place
where the optimized code has been &quot;deoptimized&quot;.</p>
<h2 id="conclusion">Conclusion</h2>
<p>The &quot;deoptimizer&quot; is really an interesting tool, and it is one of the main
cogs in <a href="http://blog.chromium.org/2010/12/new-crankshaft-for-v8.html">Crankshaft</a>&#39;s engine. This instrument helps the compiler in
executing the dynamic-language code as if it had been written in C++, because
it can always return to the slow unoptimized code with &quot;true&quot; JavaScript
semantics.</p>
<p><em>Note that things are a bit more tricky with inlined functions, but this is a
topic for another blog post.</em></p>
<p><em>Big kudos to</em>:</p>
<ul>
<li><em>Vyacheslav Egorov</em></li>
<li><em>Ben Noordhuis</em></li>
<li><em>Jeremiah Senkpiel</em></li>
</ul>
<p><em>for proof-reading this and providing valuable feedback.</em></p>
]]></description><link>http://darksi.de/a.deoptimize-me-not</link><guid isPermaLink="true">http://darksi.de/a.deoptimize-me-not</guid><dc:creator><![CDATA[Fedor Indutny]]></dc:creator><pubDate>Mon, 15 Dec 2014 00:00:00 GMT</pubDate></item><item><title><![CDATA[Cracking Cloudflare's heartbleed challenge]]></title><description><![CDATA[<h2 id="challenge">Challenge</h2>
<p>At April 11th 2014 Cloudflare has published a <a href="http://blog.cloudflare.com/answering-the-critical-question-can-you-get-private-ssl-keys-using-heartbleed">blog post</a> suggesting to
try out extracting a private key of their specially prepared
<a href="https://www.cloudflarechallenge.com/heartbleed">challenge site</a> using the <a href="http://heartbleed.com/">Heartbleed</a> OpenSSL vulnerability. Being busy
at the time, I decided to give it a try a couple of hours later, if noone would
crack it yet. This was a legal way to do some hackery, after all!</p>
<h2 id="method">Method</h2>
<p>The method of attack was following:</p>
<ol>
<li>Send a lot of random-sized fake heartbeats (without body)</li>
<li>Try to find a 128-byte prime factor of the certificate&#39;s <a href="http://en.wikipedia.org/wiki/RSA_(cryptosystem)#Key_generation">modulus</a></li>
<li>Generate the rest of the private key&#39;s parameters out of it</li>
</ol>
<p>I wasn&#39;t searching for a PEM-encoded private key and/or:</p>
<pre><code>-----BEGIN RSA PRIVATE KEY-----
</code></pre><p>for a couple of reasons:</p>
<ul>
<li>It is loaded only at the process startup</li>
<li>The key may be encrypted, and there is no point in brute forcing it</li>
</ul>
<p>According to my tests, DER-encoded key wasn&#39;t appearing in the memory either, so
trying to extract primes that are definitely in memory seem more feasible,
because they are stored in the following struct in OpenSSL:</p>
<pre><code class="lang-C">struct rsa_st
  {
  /* The first parameter is used to pickup errors where
   * this is passed instead of aEVP_PKEY, it is set to 0 */
  int pad;
  long version;
  const RSA_METHOD *meth;
  /* functional reference if &#39;meth&#39; is ENGINE-provided */
  ENGINE *engine;
  BIGNUM *n;
  BIGNUM *e;
  BIGNUM *d;
  BIGNUM *p;
  BIGNUM *q;
  BIGNUM *dmp1;
  BIGNUM *dmq1;
  BIGNUM *iqmp;
  /* be careful using this if the RSA structure is shared */
  CRYPTO_EX_DATA ex_data;
  int references;
  int flags;

  /* Used to cache montgomery values */
  BN_MONT_CTX *_method_mod_n;
  BN_MONT_CTX *_method_mod_p;
  BN_MONT_CTX *_method_mod_q;

  /* all BIGNUM values are actually in the following data, if it is not
   * NULL */
  char *bignum_data;
  BN_BLINDING *blinding;
  BN_BLINDING *mt_blinding;
  };
</code></pre>
<p>Where <code>BIGNUM</code> is:</p>
<pre><code class="lang-C">struct bignum_st
  {
  BN_ULONG *d;    /* Pointer to an array of &#39;BN_BITS2&#39; bit chunks. */
  int top;    /* Index of last used d +1. */
  /* The next are internal book keeping for bn_expand. */
  int dmax;    /* Size of the d array. */
  int neg;    /* one if the number is negative */
  int flags;
  };
</code></pre>
<p>And <code>d</code> field of <code>BIGNUM</code> is a pointer to a little-endian representation of the
number. Note that I could have been searching for a <code>dmp1</code>, <code>dmpq1</code> or <code>iqmp</code> as
well, but I was too lame at the time to put this in my tests.</p>
<h2 id="implementation">Implementation</h2>
<p>Being a node.js core developer, the platform choice for the extraction script
was obvious to me. Unfortunately, since node.js is embedding OpenSSL and
exposing only some limited amount of methods as a JavaScript API, the
<a href="https://github.com/indutny/heartbleed/blob/master/node-v0.10.26.patch">patch to add fake heartbeat methods</a> was needed. (Update: patch is no longer
needed, just install module from npm).</p>
<p>Having this at hand, the implementation was almost straightforward. It is
available as an <a href="https://github.com/indutny/heartbleed">OpenSource project on github</a> now. Here are instructions for
obtaining and using it:</p>
<pre><code class="lang-bash"># Update: patch is no longer needed, just install module from npm
git clone git://github.com/indutny/heartbleed
git clone git://github.com/joyent/node -b v0.10.26 node-hb
cd node-hb
git apply ../heartbleed/node-v0.10.26.patch
./configure --prefix=$HOME/.node/0.10.26-hb
make -j24 install
ls ./node

export PATH=&quot;$HOME/.node/0.10.26-hb/bin:$PATH&quot;

# Here it goes
npm install -g heartbleed.js

heartbleed -h cloudflarechallenge.com -c 1000 &gt;&gt; key.pem
</code></pre>
<p>Note that it won&#39;t produce any result immediately, it took me 3 hours and a
certain amount of luck to obtain the key in a Cloudflare&#39;s challenge.</p>
]]></description><link>http://darksi.de/9.heartbleed</link><guid isPermaLink="true">http://darksi.de/9.heartbleed</guid><dc:creator><![CDATA[Fedor Indutny]]></dc:creator><pubDate>Wed, 16 Apr 2014 00:00:00 GMT</pubDate></item><item><title><![CDATA[Bud - a TLS "swiss knife"]]></title><description><![CDATA[<h2 id="bud">Bud</h2>
<p>To terminate TLS or not? Good question, but instead of answering it - I&#39;ll try
to make you believe that if you need a TLS terminator - the <a href="http://github.com/indutny/bud">Bud</a> is just
the right choice.</p>
<h2 id="other-choices">Other choices</h2>
<p>Certainly, there are some other choices for TLS termination like:</p>
<ul>
<li><a href="https://github.com/voxer/stud">stud</a></li>
<li><a href="http://www.stunnel.org/">stunnel</a></li>
<li><a href="http://nginx.org/">nginx</a> (though, not only a TLS terminator, but a web server too)</li>
<li><a href="http://haproxy.1wt.eu/">haproxy</a> (much more than just a TLS terminator, but quite good!)</li>
<li>...probably some others?</li>
</ul>
<p>However, in many cases <a href="http://github.com/indutny/bud">bud</a> could do their job as well as they do and also
provide some unique features.</p>
<h2 id="features">Features</h2>
<h3 id="speed">Speed</h3>
<p>Bud is as fast as all of it rivals, here are comparison of it to <a href="https://github.com/voxer/stud">stud</a>:</p>
<p>Normal response:</p>
<p><img src="/f/bud/normal-rps.png" alt="Normal RPS">
<img src="/f/bud/normal-response.png" alt="Normal Response"></p>
<p>Big response:</p>
<p><img src="/f/bud/big-rps.png" alt="Big RPS">
<img src="/f/bud/big-response.png" alt="Big Response"></p>
<h3 id="asynchronous-sni-and-balancing">Asynchronous SNI and balancing</h3>
<p>This is a killer feature for any serious PaaS offering an HTTPS access to the
hosted applications. When enabled in configuration, on every incoming request
bud will do an http query to receive a TLS certificate/key pair and an address
of the backend to which this connection should be balanced.</p>
<p>See <a href="https://github.com/indutny/bud#sni-storage">docs</a> for details.</p>
<h3 id="asynchronous-ocsp-stapling">Asynchronous OCSP stapling</h3>
<p>The same kind of thing could be used to perform <a href="http://en.wikipedia.org/wiki/OCSP_stapling">OCSP stapling</a>
asynchronously, which is pretty useful if certificates are loaded dynamically
and it isn&#39;t possible to store all of them in memory.</p>
<p>See <a href="https://github.com/indutny/bud#ocsp-stapling">docs</a> for more details.</p>
<p>All that asynchronous APIs are JSON based, so replying to such requests is as
easy as possible for almost any platform (including node.js).</p>
<h3 id="x-forwarded-for">X-Forwarded-For</h3>
<p>The latest feature that I have implemented so far is an <code>x-forward</code> backend
option. When enabled, bud will add <code>X-Forwarded-For</code> header to the first request
of all incoming HTTP connections and send custom <code>X_FORWARD</code> frame for all
<a href="http://en.wikipedia.org/wiki/SPDY">SPDY</a> connections.</p>
<p>This custom <code>X_FORWARD</code> frame is already supported in <a href="https://www.npmjs.org/package/spdy">node-spdy@1.25.0</a> and
will automatically add <code>X-Forwarded-For</code> header to all requests on that SPDY
connection.</p>
<p>The main pros of this method is that no actual protocol parsing is happening.
The cons is that, in case of HTTP protocol, only first request gets this header
added. This could be worked around by checking this header on incoming request
and associating it with a underlying socket (<code>req.socket</code> in node.js.)</p>
<h2 id="try-it-out-">Try it out!</h2>
<p>Hearing all that awesome things - you may become interested in giving it a try,
thanks to <a href="https://npmjs.org/">npm</a> it is quite simple:</p>
<pre><code>npm install -g bud-tls
</code></pre><p>Generating a configuration is easy too:</p>
<pre><code>bud --default-config &gt; config.json
vim config.json
</code></pre><p>All this options are documented in the <a href="https://github.com/indutny/bud#bud-">project&#39;s readme</a>.</p>
<p>Just in case, this blog is running behind <a href="http://github.com/indutny/bud">bud</a>!</p>
<h2 id="reporting-issues">Reporting issues</h2>
<p>Something does not work as expected or just crashes? Please do not hesitate to
report it on <a href="https://github.com/indutny/bud/issues">github issues</a>.</p>
]]></description><link>http://darksi.de/8.bud-a-tls-swiss-knife</link><guid isPermaLink="true">http://darksi.de/8.bud-a-tls-swiss-knife</guid><dc:creator><![CDATA[Fedor Indutny]]></dc:creator><pubDate>Thu, 03 Apr 2014 00:00:00 GMT</pubDate></item><item><title><![CDATA[Running node.js + DTrace on FreeBSD]]></title><description><![CDATA[<h2 id="preface">Preface</h2>
<p>Tracing node.js activity and detecting performance problems and bottlenecks
has always been an important topic for many people in the community. Though,
various ways to do this were available, including: systemtap, ETW and perfctr on
Windows. The most complete tracing support was done by Joyent guys for the
DTrace tool which works best on their <a href="http://wiki.illumos.org/display/illumos/illumos+Home">Illumos</a> fork, called <a href="http://smartos.org/">SmartOS</a>.</p>
<p>Fortunately, since 9.0 version, FreeBSD maintainers have started fixing and
tweaking their DTrace implementation too (which is actually a backport from
Solaris). Considering that FreeBSD is much easier to install and is much
more usable as a primary OS for developers, being able to do flamegraphs for
node.js on it is something that I highly desired at the time.</p>
<h2 id="what-was-broken">What was broken</h2>
<p>Sadly, it wasn&#39;t working out-of-the-box. After the installation of FreeBSD in a
VirtualBox has finished, I immediately tried to build and dtrace node with a
<code>jstack()</code> utility (see my previous <a href="/3.dtrace-ustack-helper">blog post</a> on that topic), but it did
not work out. After some struggling it became obvious that <code>/dev/dtrace/helper</code>
has pretty narrow permissions and the node, running under non-root user, wasn&#39;t
able to register itself within the system&#39;s DTrace module.</p>
<p>Calling <code>sudo chmod 666 /dev/dtrace/helper</code> improved the situation, but not that
much: version 0.11 of node.js was crashing, and version v0.10 was still not
registering it&#39;s ustack helper and DTrace provider (see <a href="http://www.solarisinternals.com/wiki/index.php/DTrace_Topics_USDT#USDT">USDT</a> docs). This
problem was a bit tougher than the helper permissions, and took awhile to fix.</p>
<h2 id="how-node-interacts-with-dtrace">How node interacts with DTrace</h2>
<p>First of all, a bit of information about how node.js compiles it&#39;s DTrace
helpers and how they are used by the system. There were <a href="http://www.bsdcan.org/2008/schedule/attachments/60_dtrace_bsdcan.pdf">some slides</a> on that
topic (how DTrace USDT interacts with kernel), but at that moment I figured it
out by reading the implementation&#39;s source.</p>
<p>There are two <code>.d</code> files in the node.js source tree: <code>src/node_provider.d</code> and
<code>src/v8ustack.d</code>. The former declares a <code>node</code> DTrace USDT provider and the
latter exports an ustack helper Both of these files are compiled with
<code>dtrace -G -s &lt;file.d&gt; -o &lt;out.o&gt;</code>, which in fact does the following thing:</p>
<ol>
<li>Compile all D-language chunks into DIFs (<a href="https://github.com/freebsd/freebsd/blob/3ecc6f129801776dd571d69cf9a262a97ad23968/sys/cddl/contrib/opensolaris/uts/common/sys/dtrace.h#L112">DTrace Intermediate Format</a></li>
<li>Encode them in the DOF (<a href="https://github.com/freebsd/freebsd/blob/3ecc6f129801776dd571d69cf9a262a97ad23968/sys/cddl/contrib/opensolaris/uts/common/sys/dtrace.h#L570">DTrace Object File</a>) format</li>
<li>Put them into the special <code>._SUNW_dof</code> ELF section</li>
<li>Link it to the internally-stored <code>drti.o</code>, providing <code>dtrace_dof_init</code> and
<code>dtrace_dof_fini</code> helper functions.</li>
</ol>
<p>These functions are called on the executable&#39;s initialization and
deinitialization (surprisingly!), each making an <code>ioctl()</code> syscall on the
<code>/dev/dtrace/helper</code>. Specifically, <code>dtrace_dof_init()</code> loads and verifies the
<code>._SUNW_dof</code> section of an ELF-file and registers it with-in the kernel.</p>
<h2 id="how-i-fixed-it">How I fixed it</h2>
<p>I decided to investigate the node.js v0.11 crashes. It was crashing on a
<a href="https://github.com/freebsd/freebsd/blob/4d784918edbf9aefbab5ab12e4701d3104c3ff45/cddl/contrib/opensolaris/lib/libdtrace/common/drti.c#L110">this line</a>, so it was either an ELF symbol problem or a DOF string
problem. Initially, I found that the DOF did not contain the STRTAB section
that <code>dtri.o</code> was searching for, but it turned out to be a slightly bigger
problem. Since node.js has two separate <code>.d</code> files, it has two DOFs for
each of them in it&#39;s <code>._SUNW_dof</code> section, but the <code>drti.o</code> was loading only
one! After all I came up with a following patch:</p>
<script src="https://gist.github.com/indutny/d3fee964995eea206fb3.js"></script>

<p>Although, node.js v0.11 has stopped crashing after applying it to the kernel
source code and rebuilding <code>libdtrace</code>, it still wasn&#39;t registering an ustack
helper and a provider (<code>sudo dtrace -l</code> did not contain any
<code>node&lt;pid&gt;:::</code> probes).</p>
<p>While reading <a href="https://github.com/freebsd/freebsd/blob/4d784918edbf9aefbab5ab12e4701d3104c3ff45/cddl/contrib/opensolaris/lib/libdtrace/common/drti.c#L52">FreeBSD&#39;s source code</a> further, I found an environment
variable <code>DTRACE_DOF_INIT_DEBUG</code> that helped me to take a deeper look into
what was happening for both node.js v0.10 and v0.11. After setting it to
<code>DTRACE_DOF_INIT_DEBUG=1</code> node.js has started printing following things to the
stderr:</p>
<pre><code>dtrace DOF node: DTrace ioctl failed for DOF at 0x804c00000:
Argument list too long
</code></pre><p>This was totally uninformative, and I started grepping through a DTrace kernel
module with a hope to find some clues to this errors. <code>Argument list too long</code>
is a verbose description of the <code>E2BIG</code> errno, and luckily the <a href="https://github.com/freebsd/freebsd/blob/3ecc6f129801776dd571d69cf9a262a97ad23968/sys/cddl/contrib/opensolaris/uts/common/dtrace/dtrace.c#L11989">first place</a>
where it is used was the place that I needed to fix. Basically, for the security
purpose kernel limits the size of the DOF that could be loaded in it&#39;s memory.
This limit is set to the 128 KB by default, and the node.js now has
significantly bigger ustack helper (7 MB for v0.11). Instead of just raising it
to a higher value, I decided to export <code>sysctl</code> variable to make it configurable
without rebuilding the kernel. Running node again after this tweaks gave me:</p>
<pre><code>dtrace DOF node: DTrace ioctl failed for DOF at 0x804c00000:
Invalid argument
</code></pre><p>This failure was even more vague, since it meant that the <code>EINVAL</code> was returned
somewhere, and there was tons of places where it could have happened. After
inserting tons of debug prints in all possible places in kernel, I have isolated
it down to <a href="https://github.com/freebsd/freebsd/blob/3ecc6f129801776dd571d69cf9a262a97ad23968/sys/cddl/contrib/opensolaris/uts/common/dtrace/dtrace.c#L12462">this place</a>. Indeed, both of node DOFs contained a lot of
actions and the default limit (16 * 1024) was way to small for it. Exporting
another sysctl variable has solved all problems and running node.js has finally
printed this:</p>
<pre><code>dtrace DOF node: DTrace ioctl succeeded for DOF at 0x8052c2c2c
dtrace DOF node: DTrace ioctl succeeded for DOF at 0x804c00000
dtrace DOF node: found provider 0x8052c3000
</code></pre><p>Just to confirm it, I checked the <code>dtrace -l</code> output and (yikes!) it was there
too:</p>
<pre><code>48986 node909 node ... gc-done
48987 node909 node ... gc-start
48988 node909 node ... http-client-request
48989 node909 node ... http-client-response
48990 node909 node ... http-server-request
48991 node909 node ... http-server-response
48992 node909 node ... net-server-connection
48993 node909 node ... net-socket-read
48994 node909 node ... net-socket-write
48995 node909 node ... net-stream-end
</code></pre><h2 id="how-to-apply-all-this-patches">How to apply all this patches</h2>
<p>I have came up with <a href="https://github.com/indutny/freebsd/compare/release/10.0.0...feature/10.0-dtrace-patches">this instruction</a> for fixing your FreeBSD installation
to make node.js DTrace helpers work. Just a brief in-line description:</p>
<ol>
<li>Apply <a href="https://github.com/indutny/freebsd/compare/release/10.0.0...feature/10.0-dtrace-patches">these patches</a> to <code>/usr/src</code></li>
<li>Rebuild and install kernel:
<code>sudo make buildkernel &amp;&amp; sudo make installkernel</code></li>
<li>Reboot <code>sudo shutdown -r</code></li>
<li>Raise sysctl limits:<ul>
<li><code>sudo sysctl -w kern.dtrace.helper_actions_max=16000</code></li>
<li><code>sudo sysctl -w kern.dtrace.dof_maxsize=8000000</code></li>
</ul>
</li>
<li>Clone node.js:
<code>git clone git://github.com/joyent/node &amp;&amp; cd node &amp;&amp; git checkout v0.10</code></li>
<li>Configure it: <code>./configure --prefix=... --with-dtrace</code></li>
<li>Build and install it: <code>gmake -j24 &amp;&amp; gmake install</code></li>
<li>Make DTrace device accessible to non-root users:
<code>sudo chmod 666 /dev/dtrace/helper</code></li>
<li>Verify that node.js DTrace probes are inserted:
<code>DTRACE_DOF_INIT_DEBUG=1 /path/to/node</code>.</li>
</ol>
<p>Thanks for reading this, and please let me know if any of these patches don&#39;t
work for you!</p>
]]></description><link>http://darksi.de/7.freebsd-dtrace</link><guid isPermaLink="true">http://darksi.de/7.freebsd-dtrace</guid><dc:creator><![CDATA[Fedor Indutny]]></dc:creator><pubDate>Tue, 01 Apr 2014 00:00:00 GMT</pubDate></item><item><title><![CDATA[SMIs and Doubles]]></title><description><![CDATA[<p>This is a third post in the series of the JIT compiling crash-course. For a
context please consider reading <a href="/4.how-to-start-jitting">the first one</a> and <a href="/5.allocating-numbers">the second</a>.</p>
<h2 id="goal">Goal</h2>
<p>Last time we created very basic bump memory allocator and made our existing
code work with floating point double numbers, stored in the allocated heap
objects. However floating point numbers are not suitable for some of
precision-dependent operations and also, since they are stored in memory,
requiring additional memory loads and stores, slowing down the code performance.</p>
<p>Both of this problems could be solved by working with the integers stored in the
registers (as we did it in <a href="/4.how-to-start-jitting">first blog post</a>), which means that we will need
to support both types of numbers in our compiler&#39;s runtime (doubles and
integers).</p>
<h2 id="tagging">Tagging</h2>
<p>Let&#39;s recall that we are storing both pointers and numbers in the 64bit general
purpose registers (<code>rax</code>, <code>rbx</code>, ...). The main issue here is that, given some
register (say <code>rax</code>), we should be able to tell if it is a pointer to the heap
object (a &quot;boxed value&quot;) or an integer itself (an &quot;unboxed value&quot;,
Small Integer, or <em>SMI</em>).</p>
<p>Usually, a method called &quot;tagging&quot; is used to solve this. While there are
<a href="http://wingolog.org/archives/2011/05/18/value-representation-in-javascript-implementations">various ways</a> to implement tagging, including: <a href="http://evilpie.github.io/sayrer-fatval-backup/cache.aspx.htm">Nan-Boxing</a> (scroll down
to <em>Mozillas New JavaScript Value Representation</em>), Nun-Boxing, and probably
some others, our compiler will just reserve the least significant bit of the
64bit register and put <code>1</code> here if the value is a pointer and <code>0</code> if it is a
<em>SMI</em> (Small Integer).</p>
<p>Here is an example of this representation:</p>
<p><img src="/images/smi-and-pointer.png" alt="Smi and Pointer"></p>
<p>Note that to get the actual value of a SMI (&quot;untag&quot;) we will need to shift it
right for one bit (<code>&gt;&gt; 1</code>), and to convert an integer to the SMI - shift left
(<code>&lt;&lt; 1</code>). Using zero for tagging SMIs pays off greatly, since we don&#39;t need to
to untag numbers to perform addition and subtraction.</p>
<p>To use tagged pointers to heap objects we&#39;ll need to look one byte behind the
actual value, which is relatively simple in the assembly language:</p>
<pre><code class="lang-javascript">// Lets say that tagged pointer is in rbx
// And we&#39;re loading its contents into the rax
this.mov(&#39;rax&#39;, [&#39;rbx&#39;, -1]);
</code></pre>
<p>And just for the convenience - example of untagging SMIs:</p>
<pre><code class="lang-javascript">// Untag
this.shr(&#39;rax&#39;, 1);
// Tag
this.shl(&#39;rax&#39;, 1);
</code></pre>
<p>And now the most important part that we&#39;re going to do a lot - checking if the
value is a pointer:</p>
<pre><code class="lang-javascript">// Test that &#39;rax&#39; has the last bit
this.test(&#39;rax&#39;, 1);

// &#39;z&#39; stands for zero
// Basically, jump to the label if `(rax &amp; 1) == 0`
this.j(&#39;z&#39;, &#39;is-smi&#39;);

// &#39;nz&#39; stands for non-zero
// Basically, jump to the label if `(rax &amp; 1) != 0`
this.j(&#39;ne&#39;, &#39;is-heap-object-pointer&#39;);
</code></pre>
<h2 id="reworking-previous-code">Reworking previous code</h2>
<p>Using <a href="https://github.com/indutny/jit.js/tree/master/example/heap">the code from the previous blog post</a>, we can finally proceed to
implementing all this recently learned stuff.</p>
<p>First, let&#39;s add a convenient helper methods to the assembly context.</p>
<pre><code class="lang-javascript">function untagSmi(reg) {
  this.shr(reg, 1);
};

function checkSmi(value, t, f) {
  // If no `true-` and `false-` bodies were specified -
  // just test the value.
  if (!t &amp;&amp; !f)
    return this.test(value, 1);

  // Enter the scope to be able to use named labels
  this.labelScope(function() {
    // Test the value
    this.test(value, 1);

    // Skip SMI case if result is non-zero
    this.j(&#39;nz&#39;, &#39;non-smi&#39;);

    // Run SMI case
    t.call(this);

    // Jump to the shared end
    this.j(&#39;end&#39;);

    // Non-SMI case
    this.bind(&#39;non-smi&#39;);
    f.call(this);

    // Shared end
    this.bind(&#39;end&#39;);
  });
};

function heapOffset(reg, offset) {
  // NOTE: 8 is the size of pointer on x64 arch.
  // We&#39;re adding 1 to the offset, because first
  // quad word is used to store the heap object&#39;s type.
  return [reg, 8 * ((offset | 0) + 1) - 1];
};
</code></pre>
<p>We can hook this methods into the jit.js context by passing them as a <code>helpers</code>
option to the <code>jit.compile()</code> API method:</p>
<pre><code class="lang-javascript">var helpers = {
  untagSmi: untagSmi,
  checkSmi: checkSmi,
  heapOffset: heapOffset
};

jit.compile(function() {
  // We can use helpers here:
  this.untagSmi(&#39;rax&#39;);

  this.checkSmi(&#39;rbx&#39;, function() {
    // Work with SMI
  }, function() {
    // Work with pointer
  });

  this.mov(this.heapOffset(&#39;rbx&#39;, 0), 1);
}, { stubs: stubs, helpers: helpers });
</code></pre>
<h2 id="allocation">Allocation</h2>
<p>Now we should make our <code>Alloc</code> stub return tagged pointer. Also we will use the
opportunity and improve it a bit by adding <code>tag</code> and <code>size</code> arguments to the
stub (thus making possible generalized allocation with variable size and tag
in the future):</p>
<pre><code class="lang-javascript">stubs.define(&#39;Alloc&#39;, function(size, tag) {
  // Save &#39;rbx&#39; and &#39;rcx&#39; registers
  this.spill([&#39;rbx&#39;, &#39;rcx&#39;], function() {
    // Load `offset`
    //
    // NOTE: We&#39;ll use pointer to `offset` variable,
    // to be able to update
    // it below
    this.mov(&#39;rax&#39;, this.ptr(offset));
    this.mov(&#39;rax&#39;, [&#39;rax&#39;]);

    // Load end
    //
    // NOTE: Same applies to end, though, we&#39;re
    // not updating it right now
    this.mov(&#39;rbx&#39;, this.ptr(end));
    this.mov(&#39;rbx&#39;, [&#39;rbx&#39;]);

    // Calculate new `offset`
    this.mov(&#39;rcx&#39;, &#39;rax&#39;);

    // Add tag size and body size
    this.add(&#39;rcx&#39;, 8);
    this.add(&#39;rcx&#39;, size);

    // Check if we won&#39;t overflow our fixed size buffer
    this.cmp(&#39;rcx&#39;, &#39;rbx&#39;);

    // this.j() performs conditional jump to the specified label.
    // &#39;g&#39; stands for &#39;greater&#39;
    // &#39;overflow&#39; is a label name, bound below
    this.j(&#39;g&#39;, &#39;overflow&#39;);

    // Ok, we&#39;re good to go, update offset
    this.mov(&#39;rbx&#39;, this.ptr(offset));
    this.mov([&#39;rbx&#39;], &#39;rcx&#39;);

    // First 64bit pointer is reserved for &#39;tag&#39;,
    // second one is a `double` value
    this.mov(&#39;rcx&#39;, tag);
    this.mov([&#39;rax&#39;], &#39;rcx&#39;);

    // !!!!!!!!!!!!!!!
    // ! Tag pointer !
    // !!!!!!!!!!!!!!!
    this.or(&#39;rax&#39;, 1);

    // Return &#39;rax&#39;
    this.Return();

    // Overflowed :(
    this.bind(&#39;overflow&#39;)

    // Invoke javascript function!
    // NOTE: This is really funky stuff, but I&#39;m not
    // going to dive deep into it right now
    this.runtime(function() {
      console.log(&#39;GC is needed, but not implemented&#39;);
    });

    // Crash
    this.int3();

    this.Return();
  });
});
</code></pre>
<h2 id="math-stubs">Math stubs</h2>
<p>Also, as we&#39;re going to do a bit more book-keeping in math operations to support
both SMIs and doubles, let&#39;s split it apart and put the code, handling doubles
into the stub:</p>
<pre><code class="lang-javascript">var operators = [&#39;+&#39;, &#39;-&#39;, &#39;*&#39;, &#39;/&#39;];
var map = { &#39;+&#39;: &#39;addsd&#39;, &#39;-&#39;: &#39;subsd&#39;, &#39;*&#39;: &#39;mulsd&#39;,
            &#39;/&#39;: &#39;divsd&#39; };

// Define `Binary+`, `Binary-`, `Binary*`, and `Binary/` stubs
operators.forEach(function(operator) {
  stubs.define(&#39;Binary&#39; + operator, function(left, right) {
    // Save &#39;rbx&#39; and &#39;rcx&#39;
    this.spill([&#39;rbx&#39;, &#39;rcx&#39;], function() {
      // Load arguments to rax and rbx
      this.mov(&#39;rax&#39;, left);
      this.mov(&#39;rbx&#39;, right);

      // Convert both numbers to doubles
      [[&#39;rax&#39;, &#39;xmm1&#39;], [&#39;rbx&#39;, &#39;xmm2&#39;]].forEach(function(regs) {
        var nonSmi = this.label();
        var done = this.label();

        this.checkSmi(regs[0]);
        this.j(&#39;nz&#39;, nonSmi);

        // Convert integer to double
        this.untagSmi(regs[0]);
        this.cvtsi2sd(regs[1], regs[0]);

        this.j(done);
        this.bind(nonSmi);

        this.movq(regs[1], this.heapOffset(regs[0], 0));
        this.bind(done);
      }, this);

      var instr = map[operator];

      // Execute binary operation
      if (instr) {
        this[instr](&#39;xmm1&#39;, &#39;xmm2&#39;);
      } else {
        throw new Error(&#39;Unsupported binary operator: &#39; +
                        operator);
      }

      // Allocate new number, and put value in it
      // NOTE: Last two arguments are arguments to
      // the stub (`size` and `tag`)
      this.stub(&#39;rax&#39;, &#39;Alloc&#39;, 8, 1);
      this.movq(this.heapOffset(&#39;rax&#39;, 0), &#39;xmm1&#39;);
    });

    this.Return();
  });
});
</code></pre>
<p>Note that this stub also converts all incoming numbers to doubles.</p>
<h2 id="compiler">Compiler</h2>
<p>And back to the compiler&#39;s code:</p>
<pre><code class="lang-javascript">function visitProgram(ast) {
  assert.equal(ast.body.length,
               1,
               &#39;Only one statement programs are supported&#39;);
  assert.equal(ast.body[0].type, &#39;ExpressionStatement&#39;);

  // We&#39;ve a pointer in &#39;rax&#39;, convert it to integer
  visit.call(this, ast.body[0].expression);

  // Get floating point number out of heap number
  this.checkSmi(&#39;rax&#39;, function() {
    // Untag smi
    this.untagSmi(&#39;rax&#39;);
  }, function() {
    this.movq(&#39;xmm1&#39;, this.heapOffset(&#39;rax&#39;, 0));

    // Round it towards zero
    this.roundsd(&#39;zero&#39;, &#39;xmm1&#39;, &#39;xmm1&#39;);

    // Convert double to integer
    this.cvtsd2si(&#39;rax&#39;, &#39;xmm1&#39;);
  });
}

function visitLiteral(ast) {
  assert.equal(typeof ast.value, &#39;number&#39;);

  if ((ast.value | 0) === ast.value) {
    // Small Integer (SMI), Tagged value
    // (i.e. val * 2) with last bit set to
    // zero
    this.mov(&#39;rax&#39;, utils.tagSmi(ast.value));
  } else {
    // Allocate new heap number
    this.stub(&#39;rax&#39;, &#39;Alloc&#39;, 8, 1);

    // Save &#39;rbx&#39; register
    this.spill(&#39;rbx&#39;, function() {
      this.loadDouble(&#39;rbx&#39;, ast.value);

      // NOTE: Pointers have last bit set to 1
      // That&#39;s why we need to use &#39;heapOffset&#39;
      // routine to access it&#39;s memory
      this.mov(this.heapOffset(&#39;rax&#39;, 0), &#39;rbx&#39;);
    });
  }
}

function visitBinary(ast) {
  // Preserve &#39;rbx&#39; after leaving the AST node
  this.spill(&#39;rbx&#39;, function() {
    // Visit left side of expresion
    visit.call(this, ast.right);

    // Move it to &#39;rbx&#39;
    this.mov(&#39;rbx&#39;, &#39;rax&#39;);

    // Visit right side of expression (the result is in &#39;rax&#39;)
    visit.call(this, ast.left);

    //
    // So, to conclude, we&#39;ve left side in &#39;rax&#39; and right in &#39;rbx&#39;
    //

    if (ast.operator === &#39;/&#39;) {
      // Call stub for division
      this.stub(&#39;rax&#39;, &#39;Binary&#39; + ast.operator, &#39;rax&#39;, &#39;rbx&#39;);
    } else {
      this.labelScope(function() {
        // Check if both numbers are SMIs
        this.checkSmi(&#39;rax&#39;);
        this.j(&#39;nz&#39;, &#39;call stub&#39;);
        this.checkSmi(&#39;rbx&#39;);
        this.j(&#39;nz&#39;, &#39;call stub&#39;);

        // Save rax in case of overflow
        this.mov(&#39;rcx&#39;, &#39;rax&#39;);

        // NOTE: both &#39;rax&#39; and &#39;rbx&#39; are tagged at this
        // point.
        // Tags don&#39;t need to be removed if we&#39;re doing
        // addition or subtraction. However, in case of
        // multiplication result would be 2x bigger if
        // we won&#39;t untag one of the arguments.
        if (ast.operator === &#39;+&#39;) {
          this.add(&#39;rax&#39;, &#39;rbx&#39;);
        } else if (ast.operator === &#39;-&#39;) {
          this.sub(&#39;rax&#39;, &#39;rbx&#39;);
        } else if (ast.operator === &#39;*&#39;) {
          this.untagSmi(&#39;rax&#39;);
          this.mul(&#39;rbx&#39;);
        }

        // On overflow restore &#39;rax&#39; from &#39;rcx&#39; and invoke stub
        this.j(&#39;o&#39;, &#39;restore&#39;);

        // Otherwise return &#39;rax&#39;
        this.j(&#39;done&#39;);
        this.bind(&#39;restore&#39;);

        this.mov(&#39;rax&#39;, &#39;rcx&#39;);

        this.bind(&#39;call stub&#39;);

        // Invoke stub and return heap number in &#39;rax&#39;
        this.stub(&#39;rax&#39;, &#39;Binary&#39; + ast.operator, &#39;rax&#39;, &#39;rbx&#39;);

        this.bind(&#39;done&#39;);
      });
    }
  });
}

function visitUnary(ast) {
  if (ast.operator === &#39;-&#39;) {
    // Negate argument by emulating binary expression
    visit.call(this, {
      type: &#39;BinaryExpression&#39;,
      operator: &#39;*&#39;,
      left: ast.argument,
      right: { type: &#39;Literal&#39;, value: -1 }
    })
  } else {
    throw new Error(&#39;Unsupported unary operator: &#39; + ast.operator);
  }
}
</code></pre>
<p>To conclude, we are now working with SMIs by default, inlining all operations
for the speed&#39;s sake, and falling back to the doubles in case of overflow or any
other trouble, like trying to sum a double and a SMI!</p>
<p>That&#39;s all for now, see you here next time! Here is the full compiler code from
this article: <a href="https://github.com/indutny/jit.js/tree/master/example/heap-smi-and-double">github</a>. Please try cloning, running and playing with it!
Hope you enjoyed this post.</p>
]]></description><link>http://darksi.de/6.smis-and-doubles</link><guid isPermaLink="true">http://darksi.de/6.smis-and-doubles</guid><dc:creator><![CDATA[Fedor Indutny]]></dc:creator><pubDate>Thu, 14 Nov 2013 00:00:00 GMT</pubDate></item><item><title><![CDATA[Allocating numbers]]></title><description><![CDATA[<h2 id="jit">JIT</h2>
<p>This is the second blog post in the series about JIT compiling.
<a href="/4.how-to-start-jitting">The previous post</a> was an introduction into the Just-In-Time code
generation and, in particular, <a href="https://github.com/indutny/jit.js">jit.js</a> usage. If you haven&#39;t read it yet -
I recommend you to familiarize yourself with <a href="/4.how-to-start-jitting">it</a> first.</p>
<h2 id="objectives">Objectives</h2>
<p>Previously, we created a JIT compiler, supporting a very limited subset of
JavaScript: integer numbers, math binary operators (<code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>), and
<code>-</code> unary operator. This time, we will extend it by adding floating point
number support, and, to make the process funnier and to spice things up,
we will allocate and store these numbers in the heap.</p>
<p>Though, because we are doing things one step at a time, our heap won&#39;t have
Garbage Collection, and will live inside fixed sized memory chunk (say &quot;yay&quot; to
simplicity!).</p>
<h2 id="stubs">Stubs</h2>
<p>Knowing what we aim to do, we can now set up internal structures for these
features. Essentially, what we&#39;ll need is a memory allocation procedure, that
generates and returns memory addresses suitable for our goals.</p>
<p>This allocation code could be generated for every AST node using series of
inlined assembly instructions, which works great and, more importantly, is
incredibly fast for concise operations. But due to the relatively big code&#39;s
size of this procedure, the resulting machine code output may become too big to
be fit entirely into the CPU&#39;s cache, causing potential performance problems to
the whole system.</p>
<p>Generally, this is considered a bad practice. A better approach would be
parameterizing such code blocks into shared procedures called <code>stubs</code> (I picked
that naming from <a href="https://github.com/v8/v8/blob/master/src/ia32/code-stubs-ia32.cc">v8&#39;s source</a> and, perhaps, it is how these things are
named in other VMs too). For even better optimization these procedures
could be lazily compiled, i.e. we should not compile those ones that are not
used by generated code. This technique is good for both compilation time and
executable code size (and therefore CPU caches too).</p>
<p>Fortunately, <a href="https://github.com/indutny/jit.js">jit.js</a> lets you generate <em>stubs</em> easily:</p>
<pre><code class="lang-javascript">var stubs = jit.stubs();

stubs.define(&#39;Allocate&#39;, function() {
  // Our code here
  // ....

  // Returning back to caller
  this.Return();
});
</code></pre>
<p>Simple, isn&#39;t it? Now, to use it in our JIT compiler we&#39;ll need to pass it in
an options argument:</p>
<pre><code class="lang-javascript">jit.compile(function() {
  // Compiler code generation happens in this context

  // Explanation:
  // Read address of &#39;Allocate&#39; stub into &#39;rax&#39; register and
  // call it.
  this.stub(&#39;rax&#39;, &#39;Allocate&#39;);

  this.Return();
}, { stubs: stubs });
</code></pre>
<p>As mentioned above, only stubs that were used during compilation process will
actually be generated and reused between all callers.</p>
<h2 id="heap">Heap</h2>
<p>With this knowledge, we can proceed to the memory allocation phase. But first,
lets take a short look at the structure and organization of the heap.</p>
<p>The <em>heap</em> is the place where JavaScript (and many other) VMs create and store
objects (usually, ones that can&#39;t be fit into CPU registers). Some heap objects
may contain references to other objects (in other words, can reference them).
All live objects and their references create a directed graph, starting at
so called <em>roots</em> (which are usually global variables and pointers on stack).</p>
<p>Although, it is usually used in VMs with JIT compilation, Garbage Collection is
not required for the Heap. Indeed, many VMs and languages choose to use
unmanaged memory instead (C/C++ as a banal example). In such cases you (as the
language user) will generally need to explicitly free unused resources to not
run out of the memory.</p>
<p>But for obvious reasons, the JavaScript subset compiler that we&#39;re implementing,
should support both managed memory and Garbage Collection (which will be
implemented later).</p>
<p>There are tons of books that may give you an advanced introduction into the
heap allocation and garbage collection (my recommendation is
<a href="http://www.amazon.com/The-Garbage-Collection-Handbook-Management/dp/1420082795/ref=sr_1_1?ie=UTF8&amp;qid=1383600127&amp;sr=8-1&amp;keywords=garbage+collection+handbook">The Garbage Collection Handbook</a>), and considerably many ways to allocate
and collect memory in the heap.</p>
<p>Usually, you will need to choose between the allocation speed and memory
fragmentation. But, since we are not covering this very deeply, I would
recommend to stick with the method called &quot;bump allocation&quot; for now.</p>
<h2 id="bump-allocation">Bump allocation</h2>
<p>Fixed-page bump allocation works in a following way.</p>
<ol>
<li>Take the memory chunk of fixed size (a <em>page</em>)</li>
<li>Give away consequent slices of it as a return value of the allocation
procedure.</li>
<li>When running low on memory, perform the Garbage Collection and free all
unused space, by either compacting live objects or evacuating them to the
new memory chunk (replacing references to live objects in both cases).</li>
</ol>
<p>In terms of <a href="https://github.com/indutny/jit.js">jit.js</a> and stubs API, this procedure may look as following:</p>
<pre><code class="lang-javascript">// Create fixed size memory chunk
var page = new Buffer(1024);

// Set-up pointers to page start and page end
var offset = jit.ptr(page);
var end = jit.ptr(page, page.length);

stubs.define(&#39;Alloc&#39;, function() {

  // Save &#39;rbx&#39; and &#39;rcx&#39; registers
  this.spill([&#39;rbx&#39;, &#39;rcx&#39;], function() {
    // Load `offset`
    //
    // NOTE: We&#39;ll use pointer to `offset` variable, to be able to update
    // it below
    this.mov(&#39;rax&#39;, this.ptr(offset));
    this.mov(&#39;rax&#39;, [&#39;rax&#39;]);

    // Load end
    //
    // NOTE: Same applies to end, though, we&#39;re not updating it right now
    this.mov(&#39;rbx&#39;, this.ptr(end));
    this.mov(&#39;rbx&#39;, [&#39;rbx&#39;]);

    // Calculate new `offset`
    this.mov(&#39;rcx&#39;, &#39;rax&#39;);

    // We&#39;ll assume that all allocations are 16 bytes = two 64bit pointers
    this.add(&#39;rcx&#39;, 16);

    // Check if we won&#39;t overflow our fixed size buffer
    this.cmp(&#39;rcx&#39;, &#39;rbx&#39;);

    // this.j() performs conditional jump to the specified label.
    // &#39;g&#39; stands for &#39;greater&#39;
    // &#39;overflow&#39; is a label name, bound below
    this.j(&#39;g&#39;, &#39;overflow&#39;);

    // Ok, we&#39;re good to go, update offset
    this.mov(&#39;rbx&#39;, this.ptr(offset));
    this.mov([&#39;rbx&#39;], &#39;rcx&#39;);

    // The first 64bit pointer is reserved for &#39;tag&#39;,
    // the second one is a `double` value
    this.mov([&#39;rax&#39;], 1);

    // Return &#39;rax&#39;
    this.Return();

    // Overflowed :(
    this.bind(&#39;overflow&#39;)

    // Invoke javascript function!
    // NOTE: This is really funky stuff, but I&#39;m not going to dive deep
    // into it right now
    this.runtime(function() {
      console.log(&#39;GC is needed, but not implemented&#39;);
    });

    // Crash
    this.int3();

    this.Return();
  });
});
</code></pre>
<p>That&#39;s it! Not totally straightforward, but not really complicated either!</p>
<p>This procedure will give away consequent slices of the <em>page</em>, and even tag
them! (I&#39;ll cover tagging in one of the next posts. Basically, they&#39;re used to
distinguish different kinds of heap objects).</p>
<p>Few things to note here:</p>
<ol>
<li><code>jit.ptr(buf, offset)</code> returns a <code>Buffer</code>, containing a pointer to the given
<code>buf</code> with <code>offset</code> added to it.</li>
<li><code>this.spill()</code> is a routine for saving and restoring registers to/from the
 memory (this process is usually called <em>spilling</em>). It takes list of the
 registers and the closure. These registers will be saved before entering the
 closure, and restored right after leaving it.
 NOTE: The restore code will be generated before each <code>this.Return()</code> too.</li>
<li><code>this.mov([&#39;rbx&#39;], &#39;rcx&#39;)</code> - stores <code>rcx</code> register into the memory location,
pointed by the value of <code>rbx</code> register.
NOTE: you can also specify an offset here: <code>this.mov([&#39;rbx&#39;, 8], &#39;rcx&#39;)</code>.</li>
<li>jit.js supports branching primitives: <code>this.cmp(a, b)</code>,
<code>this.j(condition, labelName)</code>, <code>this.j(labelName)</code>, <code>this.bind(labelName)</code>.</li>
</ol>
<h1 id="floating-point">Floating point</h1>
<p>Now as we have a <em>presumably</em> working allocation procedure, let&#39;s recall what
should be stored inside of this heap chunks. In the allocation procedure, we
create chunks with the 8 byte tag value, and the 8 byte contents. This is
enough to store <code>double</code> (as C type) floating point numbers.</p>
<p>There are plenty of assembly instructions to load/store/work with such numbers.
But note that to work with them - you&#39;ll need to store them in the different
register set: <code>xmm0</code>, <code>xmm1</code>, ... <code>xmm15</code>. Although, 64-bit floating numbers
could be stored in the general purpose registers: <code>rax</code>, <code>rbx</code>, ... Performing
math operations is possible only with a <code>xmm</code> register set. Here are some
instructions, that are present in <code>jit.js</code> and should be useful for our
compiler:</p>
<ol>
<li><code>movq(&#39;xmm&#39;, &#39;gp&#39;)</code> or <code>movq(&#39;gp&#39;, &#39;xmm&#39;)</code> to move 64bits from the general
purpose register (or memory pointed by it) to xmm, or the other way around.</li>
<li><code>movsd(&#39;xmm&#39;, &#39;xmm&#39;)</code> to move the value from one xmm to another.</li>
<li><code>addsd</code>, <code>mulsd</code>, <code>subsd</code>, <code>divsd</code> - addition, multiplication, subtraction,
division.</li>
<li><code>cvtsi2sd(&#39;xmm&#39;, &#39;gp&#39;)</code>, <code>cvts2si(&#39;gp&#39;, &#39;xmm&#39;)</code> - converts integer into
double, and double into integer, respectively.</li>
<li><code>roundsd(&#39;mode&#39;, &#39;xmm&#39;, &#39;xmm&#39;)</code> - round the <code>src</code> register using specified
<code>mode</code> (which is one of: <code>nearest</code>, <code>down</code>, <code>up</code>, <code>zero</code>) and place the
result into the <code>dst</code> register.</li>
</ol>
<p>Using this sacred knowledge we can patch our existing code to make it work with
the floating point numbers (yeah, we will remove the integer support for now):</p>
<pre><code class="lang-javascript">// Compile
var fn = jit.compile(function() {
  // This will generate default entry boilerplate
  this.Proc(function() {
    visit.call(this, ast);

    // The result should be in &#39;rax&#39; at this point
    //
    // This will generate default exit boilerplate
    this.Return();
  });
}, { stubs: stubs });

// Execute
console.log(fn());

function visit(ast) {
  if (ast.type === &#39;Program&#39;)
    visitProgram.call(this, ast);
  else if (ast.type === &#39;Literal&#39;)
    visitLiteral.call(this, ast);
  else if (ast.type === &#39;UnaryExpression&#39;)
    visitUnary.call(this, ast);
  else if (ast.type === &#39;BinaryExpression&#39;)
    visitBinary.call(this, ast);
  else
    throw new Error(&#39;Unknown ast node: &#39; + ast.type);
}

function visitProgram(ast) {
  assert.equal(ast.body.length,
               1,
               &#39;Only one statement programs are supported&#39;);
  assert.equal(ast.body[0].type, &#39;ExpressionStatement&#39;);

  // We&#39;ve a pointer in &#39;rax&#39;, convert it to integer
  visit.call(this, ast.body[0].expression);

  // Get floating point number out of heap number
  this.movq(&#39;xmm1&#39;, [&#39;rax&#39;, 8]);

  // Round it towards zero
  this.roundsd(&#39;zero&#39;, &#39;xmm1&#39;, &#39;xmm1&#39;);

  // Convert double to integer
  this.cvtsd2si(&#39;rax&#39;, &#39;xmm1&#39;);
}

function visitLiteral(ast) {
  assert.equal(typeof ast.value, &#39;number&#39;);

  // Allocate new heap number
  this.stub(&#39;rax&#39;, &#39;Alloc&#39;);

  // Save &#39;rbx&#39; register
  this.spill(&#39;rbx&#39;, function() {
    this.loadDouble(&#39;rbx&#39;, ast.value);
    this.mov([&#39;rax&#39;, 8], &#39;rbx&#39;);
  });
}

function visitBinary(ast) {
  // Preserve &#39;rbx&#39; after leaving the AST node
  this.spill(&#39;rbx&#39;, function() {
    // Visit right side of expresion
    visit.call(this, ast.right);

    // Move it to &#39;rbx&#39;
    this.mov(&#39;rbx&#39;, &#39;rax&#39;);

    // Visit left side of expression (the result is in &#39;rax&#39;)
    visit.call(this, ast.left);

    //
    // So, to conclude, we&#39;ve left side in &#39;rax&#39; and right in &#39;rbx&#39;
    //

    // Let&#39;s load their double values
    this.movq(&#39;xmm1&#39;, [&#39;rax&#39;, 8]);
    this.movq(&#39;xmm2&#39;, [&#39;rbx&#39;, 8]);

    // Execute binary operation
    if (ast.operator === &#39;+&#39;) {
      this.addsd(&#39;xmm1&#39;, &#39;xmm2&#39;);
    } else if (ast.operator === &#39;-&#39;) {
      this.subsd(&#39;xmm1&#39;, &#39;xmm2&#39;);
    } else if (ast.operator === &#39;*&#39;) {
      this.mulsd(&#39;xmm1&#39;, &#39;xmm2&#39;);
    } else if (ast.operator === &#39;/&#39;) {
      this.divsd(&#39;xmm1&#39;, &#39;xmm2&#39;);
    } else {
      throw new Error(&#39;Unsupported binary operator: &#39; + ast.operator);
    }

    // Allocate new number, and put value in it
    this.stub(&#39;rax&#39;, &#39;Alloc&#39;);
    this.movq([&#39;rax&#39;, 8], &#39;xmm1&#39;);
  });
}

function visitUnary(ast) {
  if (ast.operator === &#39;-&#39;) {
    // Negate argument by emulating binary expression
    visit.call(this, {
      type: &#39;BinaryExpression&#39;,
      operator: &#39;*&#39;,
      left: ast.argument,
      right: { type: &#39;Literal&#39;, value: -1 }
    })
  } else {
    throw new Error(&#39;Unsupported unary operator: &#39; + ast.operator);
  }
}
</code></pre>
<h2 id="to-be-continued">To be continued</h2>
<p>So, that&#39;s all I have to say to you for now. On a more social theme, you may
want subscribe to my <a href="https://twitter.com/indutny">twitter</a> or watch my <a href="https://github.com/indutny/blog">blog on github</a>. Don&#39;t miss
the next post!</p>
]]></description><link>http://darksi.de/5.allocating-numbers</link><guid isPermaLink="true">http://darksi.de/5.allocating-numbers</guid><dc:creator><![CDATA[Fedor Indutny]]></dc:creator><pubDate>Wed, 06 Nov 2013 00:00:00 GMT</pubDate></item><item><title><![CDATA[How to start JIT-ting]]></title><description><![CDATA[<h2 id="premise">Premise</h2>
<p>Most developers heard about JIT compilers and how they can make slow interpreted
languages run at a speed, comparable to native code. However, not many people
understand how exactly this JIT thing works, and even less people could
write their own compilers.</p>
<p>I think having at least, basic knowledge of compiler internals may greatly
improve understanding of the code that is running on that software.</p>
<p>In this article, we&#39;ll visit some peaks of JIT-island, and probably even
implement a compiler ourselves!</p>
<h2 id="what-we-ll-start-with">What we&#39;ll start with</h2>
<p>Knowing some compiler basics, we can assume that every compiler is
transforming input in some format (usually, a source code) into the output in
another or same format (usually, a machine code). JIT compilers are not an
exception.</p>
<p>What really makes them exceptional, is the fact that they&#39;re running not ahead
of time (like gcc, clang and others), but Just-In-Time (i.e. right before
executing compiler&#39;s output).</p>
<p>To start developing our own JIT compiler we&#39;ll need to select the input language
for it. Considering <a href="http://adambard.com/blog/top-github-languages-for-2013-so-far/">TOP GITHUB LANGUAGES FOR 2013 (SO FAR)</a>, JavaScript
seems like a good candidate for implementing some limited subset of
it with simplified semantics. Even more, we&#39;ll implement JIT compiler in the
JavaScript itself. You can call it META-META!</p>
<h2 id="ast">AST</h2>
<p>Our compiler will accept JavaScript source code as its input, and produce (and
immediately execute) machine code for the very popular X64 platform. But, while
its pretty comfortable for humans to work with a textual representation,
compiler developers are usually tending to create multiple Intermediate
Representations (IR) before generating the final machine code.</p>
<p>Since we&#39;re writing simplified compiler, having only one IR should be enough for
us, and I&#39;ll choose Abstract Syntax Tree (AST) representation for this purposes.</p>
<p>Getting AST out of JavaScript code is really easy nowadays, and we can choose
any (of dozens) library we like: <a href="https://github.com/ariya/esprima">esprima</a>, <a href="https://github.com/ariya/esprima">uglify-js</a>, etc. Just to be
on one page with me, I recommend you to choose <a href="https://github.com/ariya/esprima">esprima</a>. It has a nice and
well defined <a href="https://developer.mozilla.org/en-US/docs/SpiderMonkey/Parser_API">output format</a>.</p>
<p>For example, this code: <code>obj.method(42)</code> will produce the following AST (using
<code>esprima.parse(&quot;...&quot;)</code>):</p>
<pre><code class="lang-javascript">{ type: &#39;Program&#39;,
  body:
   [ { type: &#39;ExpressionStatement&#39;,
       expression:
        { type: &#39;CallExpression&#39;,
          callee:
           { type: &#39;MemberExpression&#39;,
             computed: false,
             object: { type: &#39;Identifier&#39;, name: &#39;obj&#39; },
             property: { type: &#39;Identifier&#39;, name: &#39;method&#39; } },
          arguments: [ { type: &#39;Literal&#39;, value: 42 } ] } } ] }
</code></pre>
<h2 id="machine-code">Machine code</h2>
<p>Let&#39;s summarize: we have JavaScript source (<em>check</em>), its AST (<em>check</em>), and we
want to get machine code for it.</p>
<p>If you&#39;re already familiar with assembly language then you can skip this
chapter, as it contains only basic introductionary material on this topic.
However, if you&#39;re new to it, reading next chapter may be hard without learning
some basics first. So please stay here, it won&#39;t take too long!</p>
<p>Assembly language is the nearest textual representation of the binary code that
your CPU(s) understand and is(are) able to run. Considering that processors are
executing code by reading and running instructions one-by-one, it may seem
logical to you that almost every line in assembly program represent an
instruction:</p>
<pre><code class="lang-asm">mov rax, 1    ; Put 1 into the register named `rax`
mov rbx, 2    ; Put 2 into the register named `rbx`
add rax, rbx  ; Calculate sum of `rax` and `rbx` and put it into `rax`
</code></pre>
<p>This program&#39;s output (assuming you&#39;ll get it from <code>rax</code> register) is 3. And,
as you&#39;ve probably already figured out, it puts some data in some CPU slots
(<a href="http://en.wikipedia.org/wiki/Processor_register">registers</a>) and asks the CPU to calculate the sum of them.</p>
<p>Usually processors have enough registers to store results of intermediate
operations, but in some situations you may want to store/load data (and work
with it) from the computer&#39;s memory:</p>
<pre><code class="lang-asm">mov rax, 1
mov [rbp-8], rbx  ; Save rbx register into a stack slot
mov rbx, 2
add rax, rbx
mov rbx, [rbp-8]  ; Restore rbx register from a stack slot
</code></pre>
<p>Registers have names, memory slots have addresses. These addresses are usually
written using <code>[...]</code> syntax. For example, <code>[rbp-8]</code> means: take the value of
the <code>rbp</code> register, subtract <code>8</code>, and access a memory slot using the resulting
value as the address.</p>
<p>You can see that we&#39;re using <code>rbp</code> register here. <code>rbp</code> usually contains
address at which on-stack variables storage (i.e. variables that are stored in
current procedure&#39;s <a href="http://en.wikipedia.org/wiki/Stack_(abstract_data_type)">stack</a>) starts; <code>8</code> is a size of <code>rbx</code> register (and any
other register, prefixed with <code>r</code>), and since the <a href="http://en.wikipedia.org/wiki/Stack_(abstract_data_type)">stack</a> is growing upwards,
we need to subtract it from <code>rbp</code> to get a free address slot for our purposes.</p>
<p>There are many more nuances of programming at such a low level, and
unfortunately I&#39;m not going to cover all of them here. Also, please be aware
that I gave you a very shallow description, and what actually happens here may
sometimes be much more complex.</p>
<p>Knowing things mentioned above should be enough to proceed to the code
generation.</p>
<h2 id="code-generation">Code generation</h2>
<p>Implementing the entire JavaScript is a rather complicated practice, so we&#39;ll
implement only a simplified arithmetics engine for now. (Which should be as fun
as getting to the whole thing later!)</p>
<p>The best and the easiest way to do it, is to traverse the AST using
<a href="http://en.wikipedia.org/wiki/Depth-first_search">Depth First Search</a>, generating machine code for each node. You might wonder
how could you generate machine code in a memory-safe language like JavaScript.
That&#39;s where I&#39;m going to introduce you to <a href="https://github.com/indutny/jit.js">jit.js</a>.</p>
<p>It is a node.js module (and C++ addon, actually) capable of generating and
execution of machine code, using assembly-like JavaScript syntax:</p>
<pre><code class="lang-javascript">var jit = require(&#39;jit.js&#39;);

var fn = jit.compile(function() {
  this.Proc(function() {
    this.mov(&#39;rax&#39;, 42);
    this.Return();
  });
});
console.log(fn());  // 42
</code></pre>
<h2 id="let-s-write-it">Let&#39;s write it</h2>
<p>Thus only one thing left now, a module to traverse the AST tree, generated by
<a href="https://github.com/ariya/esprima">esprima</a>. Thankfully, considering its structure and our minimalistic
compiler design it should be pretty easy.</p>
<p>We&#39;re going to support:</p>
<ol>
<li>Number literals (<code>{ type: &#39;Literal&#39;, value: 123 }</code>)</li>
<li>Binary expression, with operators: <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, <code>%</code>
(<code>{ type: &#39;BinaryExpression&#39;, operator: &#39;+&#39;, left: ... , right: .... }</code>)</li>
<li>Unary expression, with the <code>-</code> operator
(<code>{ type: &#39;UnaryExpression&#39;, operator: &#39;-&#39;, argument: ... }</code>)</li>
</ol>
<p>All these operations are performed on integers, so don&#39;t expect it to work
properly with values like <code>0.5</code>, <code>0.66666</code>, etc.</p>
<p>While processing expression, we&#39;ll be visiting each supported AST node of it,
generating code that returns it&#39;s result in the <code>rax</code> register. Sounds easy,
right? The only rule here is that we should keep all other registers clean
after leaving the AST node. Which, in other words, means that we should save all
registers that are used and restore them after they&#39;re not needed anymore.
Fortunately, CPUs have two magic instructions <code>push</code> and <code>pop</code> that can help us
with that task.</p>
<p>Here is the resulting code with descriptive comments:</p>
<pre><code class="lang-javascript">var jit = require(&#39;jit.js&#39;),
    esprima = require(&#39;esprima&#39;),
    assert = require(&#39;assert&#39;);

var ast = esprima.parse(process.argv[2]);

// Compile
var fn = jit.compile(function() {
  // This will generate default entry boilerplate
  this.Proc(function() {
    visit.call(this, ast);

    // The result should be in &#39;rax&#39; at this point

    // This will generate default exit boilerplate
    this.Return();
  });
});

// Execute
console.log(fn());

function visit(ast) {
  if (ast.type === &#39;Program&#39;)
    visitProgram.call(this, ast);
  else if (ast.type === &#39;Literal&#39;)
    visitLiteral.call(this, ast);
  else if (ast.type === &#39;UnaryExpression&#39;)
    visitUnary.call(this, ast);
  else if (ast.type === &#39;BinaryExpression&#39;)
    visitBinary.call(this, ast);
  else
    throw new Error(&#39;Unknown ast node: &#39; + ast.type);
}

function visitProgram(ast) {
  assert.equal(ast.body.length,
               1,
               &#39;Only one statement programs are supported&#39;);
  assert.equal(ast.body[0].type, &#39;ExpressionStatement&#39;);
  visit.call(this, ast.body[0].expression);
}

function visitLiteral(ast) {
  assert.equal(typeof ast.value, &#39;number&#39;);
  assert.equal(ast.value | 0,
               ast.value,
               &#39;Only integer numbers are supported&#39;);

  this.mov(&#39;rax&#39;, ast.value);
}

function visitBinary(ast) {
  // Preserve &#39;rbx&#39; after leaving the AST node
  this.push(&#39;rbx&#39;);

  // Visit right side of expresion
  visit.call(this, ast.right);

  // Move it to &#39;rbx&#39;
  this.mov(&#39;rbx&#39;, &#39;rax&#39;);

  // Visit left side of expression (the result is in &#39;rax&#39;)
  visit.call(this, ast.left);

  //
  // So, to conclude, we&#39;ve left side in &#39;rax&#39; and right in &#39;rbx&#39;
  //

  // Execute binary operation
  if (ast.operator === &#39;+&#39;) {
    this.add(&#39;rax&#39;, &#39;rbx&#39;);
  } else if (ast.operator === &#39;-&#39;) {
    this.sub(&#39;rax&#39;, &#39;rbx&#39;);
  } else if (ast.operator === &#39;*&#39;) {
    // Signed multiplication
    // rax = rax * rbx
    this.imul(&#39;rbx&#39;);
  } else if (ast.operator === &#39;/&#39;) {
    // Preserve &#39;rdx&#39;
    this.push(&#39;rdx&#39;);

    // idiv is dividing rdx:rax by rbx, therefore we need to clear rdx
    // before running it
    this.xor(&#39;rdx&#39;, &#39;rdx&#39;);

    // Signed division, rax = rax / rbx
    this.idiv(&#39;rbx&#39;);

    // Restore &#39;rdx&#39;
    this.pop(&#39;rdx&#39;);
  } else if (ast.operator === &#39;%&#39;) {
    // Preserve &#39;rdx&#39;
    this.push(&#39;rdx&#39;);

    // Prepare to execute idiv
    this.xor(&#39;rdx&#39;, &#39;rdx&#39;);
    this.idiv(&#39;rbx&#39;);

    // idiv puts remainder in &#39;rdx&#39;
    this.mov(&#39;rax&#39;, &#39;rdx&#39;);

    // Restore &#39;rdx&#39;
    this.pop(&#39;rdx&#39;);
  } else {
    throw new Error(&#39;Unsupported binary operator: &#39; + ast.operator);
  }

  // Restore &#39;rbx&#39;
  this.pop(&#39;rbx&#39;);

  // The result is in &#39;rax&#39;
}

function visitUnary(ast) {
  // Visit argument and put result into &#39;rax&#39;
  visit.call(this, ast.argument);

  if (ast.operator === &#39;-&#39;) {
    // Negate argument
    this.neg(&#39;rax&#39;);
  } else {
    throw new Error(&#39;Unsupported unary operator: &#39; + ast.operator);
  }
}
</code></pre>
<p>You can try it by cloning it from <a href="https://github.com/indutny/jit.js/tree/master/example/basic">github</a>, running <code>npm install</code> in it&#39;s
folder and then voila!</p>
<pre><code class="lang-bash">$ node ./main.js &#39;1 + 2 * 3&#39;
7
</code></pre>
<p>Thanks for reading up to this point! I&#39;ll talk about floating point operations
and the heap in the next blog post!</p>
]]></description><link>http://darksi.de/4.how-to-start-jitting</link><guid isPermaLink="true">http://darksi.de/4.how-to-start-jitting</guid><dc:creator><![CDATA[Fedor Indutny]]></dc:creator><pubDate>Fri, 01 Nov 2013 00:00:00 GMT</pubDate></item><item><title><![CDATA[DTrace and the little ustack helper that could]]></title><description><![CDATA[<p><a target=__blank href="/f/tlsnappy-x64.svg">
  <img src="/images/flamegraph.png" alt="Flamegraph">
</a></p>
<p><a href="http://blog.nodejs.org/2012/04/25/profiling-node-js/">Flamegraphs</a> are awesome if you need to profile your node.js application.
They provide a nice looking visual view of where your application is spending
its time. Although they&#39;re <a href="http://blog.nodejs.org/2012/04/25/profiling-node-js/">well</a> <a href="http://dtrace.org/blogs/dap/2012/01/05/where-does-your-node-program-spend-its-time/">documented</a>, no one has ever said a
word on how they work internally, but everyone mentions
&quot;ustack helper&quot; which, right now, works only on SmartOS.</p>
<h1 id="call-stack">Call stack</h1>
<p>To understand profiling, one must understand what a callstack is. During its
lifetime every application is using <a href="http://en.wikipedia.org/wiki/Stack_(abstract_data_type)">stack</a>, which is a chunk of memory which can
be changed by using <code>push</code>, <code>pop</code>, <code>call</code> and other CPU instructions, or
by accessing it directly.</p>
<p>The <code>push</code> and <code>pop</code> instructions simply expand/shrink stack storing/loading
data on top of it. The <code>call</code> instruction is a little bit more interesting:</p>
<p><em>(Quote from
  <a href="http://download.intel.com/products/processor/manual/325462.pdf">Intel 64 and IA-32 Architectures Software Developers Manual</a>)</em></p>
<pre><code>...the processor pushes the value of the EIP register (which contains the
offset of the instruction following the CALL instruction) on the stack (for
use later as a return-instruction pointer). The processor then branches to
the address in the current code segment specified by the target operand.
</code></pre><p>So coupled with the <code>ret</code> instruction <code>call</code> allows you to jump into some
function and return back to the place where it was called. (Despite it&#39;s
simplicity, I still find it amazing.)</p>
<p>That&#39;s how calling functions really work internally, but stack can be also used
to store local (on-stack) function&#39;s data. This is achieved using stack frames.
This is how functions&#39; assembly code do usually look:</p>
<p><em>(I&#39;ll use <a href="http://en.wikipedia.org/wiki/X86_assembly_language#Syntax">AT&amp;T assembly syntax</a>)</em></p>
<pre><code>push ebp ; Save previous frame pointer
mov  esp, ebp ; Set new frame pointer
sub  $0x60, esp ; Allocate space on stack

; Function&#39;s body.
mov  $0x10, -0x8(%ebp) ; set on-stack variable

mov  ebp, esp ; Shrink stack to it&#39;s initial value
pop  ebp ; Restore previous frame pointer
ret  0 ; Return to caller
</code></pre><p>If represented graphically stack generally looks like this:</p>
<p><img src="/images/callstack.png" alt="Callstack"></p>
<p>The main pros of using structure above are:</p>
<ul>
<li>Easiness of restoring stack back to its initial position</li>
<li>Fast and simple access to on-stack variables</li>
</ul>
<h1 id="stack-trace">Stack trace</h1>
<p>Suppose your application has thrown an exception or crashed with a segmentation
fault. To find the cause of the problem one may start by looking at the stack
trace where the crash has happened:</p>
<pre><code>#0  0x00007fff84356d16 in kevent ()
#1  0x00000001000557b7 in kqueue_poll ()
#2  0x000000010004c77a in uv__run ()
#3  0x000000010004c92a in uv_run ()
#4  0x0000000100015319 in node::Start ()
#5  0x000000010000dd24 in start ()
</code></pre><p>Here, on the left, you can see addresses of functions&#39; code. Debugger gets them
by taking current <code>eip</code> and <code>ebp</code> registers (which stands for current
instruction address and current stack frame address), walking stack frames and
collecting return addresses from it. On the right side, you can see functions&#39;
real names. <a href="http://www.gnu.org/software/gdb/">gdb</a> automatically loads this information for you by searching
for debugging symbols corresponding to addresses it has collected.</p>
<h1 id="flamegraph">Flamegraph</h1>
<p>In order to create flamegraph, one will need to periodically collect
application&#39;s stack traces and join them (the process is called
<a href="http://en.wikipedia.org/wiki/Profiling_(computer_programming)#Statistical_profilers">statistical profiling</a>),  making boxes with functions that were called more
often - wider, and putting box on the top of another box only if their functions
appear above each other in the stack trace.</p>
<h1 id="v8-s-stack-frames">V8&#39;s stack frames</h1>
<p>When collecting stack traces of C/C++ application, dtrace will use static
debugging information using binary&#39;s symbols table. But when it comes to dynamic
languages, getting such information turns out to be more complicated:
functions are compiled lazily, often recompiled with applied optimizations, old
code may be evicted by GC... in other words, application is evolving during its
execution.</p>
<p>Thankfully, V8 provides this information, but instead of debugging symbols
it stores it in stack frames. Here is an example of v8&#39;s stack frame structure:</p>
<p><img src="/images/v8-callstack.png" alt="V8 Callstack"></p>
<p>So knowing this structure we can identify frames by checking marker/function
value and getting function names from V8&#39;s heap (it&#39;s too big topic to cover
here, believe me).</p>
<p>That&#39;s exactly the job of ustack helper, it takes frame address and should figure
out and return function&#39;s name, or just fail. So everytime you call <code>jstack()</code>
function in DTrace probe, ustack helper will be called for every unidentified
frame.</p>
<h1 id="ustack-helper-example">ustack helper example</h1>
<p><em>NOTE: some knowledge of D language is required to fully understand code below</em></p>
<pre><code>dtrace:helper:ustack:
{
  /* frame pointer */
  this-&gt;fp = arg1;

  /* Last statement - result */
  &quot;whoa! you&#39;ve identified me&quot;;
}
</code></pre><p>If you replace contents of <code>src/v8ustack.d</code> in node.js sources, recompile it
(on SmartOS), run <code>bash benchmark/http-flamegraph.sh</code>, and open <code>stacks.src</code>,
which should contain following stack traces:</p>
<pre><code>node`_ZN2v88internal7Context14native_contextEv
node`_ZN4node10StreamWrap15WriteStringImplILNS_13...
node`_ZN4node10StreamWrap15WriteUtf8StringERKN2v89ArgumentsE+0x9
whoa! you&#39;ve identified me
whoa! you&#39;ve identified me
whoa! you&#39;ve identified me
</code></pre><p>As you can see, DTrace has identified some C++ functions and for all other
addresses has called our ustack helper.</p>
<p>Let&#39;s read some data from V8&#39;s stack frame:</p>
<pre><code>#define FP_MARKER (-2 * 8)
#define FT_ENTRY (1 &lt;&lt; 32)

/* Init */
dtrace:helper:ustack:
{
  this-&gt;fp = arg1;
  this-&gt;done = 0;
  this-&gt;marker = (uint64_t) 0;
}

/* Get marker */
dtrace:helper:ustack:
{
  this-&gt;marker = *(uint64_t*) copyin(this-&gt;fp + FP_MARKER,
                                     sizeof(uint64_t));
}

/* Match entry marker */
dtrace:helper:ustack:
/this-&gt;marker == FT_ENTRY/
{
  this-&gt;done = 1;
  &quot;entry&quot;;
}

/* Match everything else */
dtrace:helper:ustack:
/!this-&gt;done/
{
  &quot;everything else&quot;;
}
</code></pre><p>Run it again, and if you&#39;re lucky enough you&#39;ll find this in <code>stacks.src</code>:</p>
<pre><code>everything else
everything else
entry
node`_ZN2v88internalL6InvokeEbNS0...
</code></pre><p>Important things about ustack helper:</p>
<ul>
<li>It&#39;s running within kernel (though, in it&#39;s own context, so it can&#39;t crash
it). The most important consequences of it is that user-land addresses can&#39;t
be accessed directly, but only by using <code>copyin()</code> function.</li>
<li>Usage of control flow statements (if/foreach/while) in DTrace scripts is
prohibited, since all probes should terminate in a reasonable time. Otherwise
infinite loop in kernel space will cause your system to halt.</li>
</ul>
<h1 id="debugging-ustack-helper">Debugging ustack helper</h1>
<p>During development of 64bit platform support for node.js ustack helper, I found
that it&#39;s pretty hard to debug ustack helper. The only method to do this is
insertion of probes which are returning some debugging information, and
observing this information later in stack traces.</p>
<p>Additionally, it&#39;s worth noting that failed <code>copyin()</code> or any bad memory access
won&#39;t produce any informative output, but you&#39;ll see raw address in stack trace
(i.e. 0x0000000012345678) rather than your pretty real function&#39;s name.</p>
<h1 id="epilogue">Epilogue</h1>
<p>You can look at/play with node&#39;s <a href="https://github.com/joyent/node/blob/master/src/v8ustack.d">ustack helper</a>, big kudos to
<a href="https://github.com/davepacheco">Dave Pacheco</a> for developing it!</p>
<p>And you should check out Bryan Cantrill&#39;s and Dave Pacheco&#39;s presentation that
explains many things that wasn&#39;t covered in this post:
<a href="http://www.slideshare.net/bcantrill/goto2012">Dynamic Languages in Production: Progress and Open Challenges</a> and
<a href="http://www.livestream.com/dataweek/video?clipId=pla_59016422-9a89-45be-ac86-64bc4c45fe99&amp;utm_source=lslibrary&amp;utm_medium=ui-thumb">video</a>.</p>
<p>Huge thanks to <a href="http://voxer.com/">Voxer</a> for funding my investigation and work on porting
DTrace ustack helper to 64bit platform! Guys, I love you. You&#39;re awesome!</p>
]]></description><link>http://darksi.de/3.dtrace-ustack-helper</link><guid isPermaLink="true">http://darksi.de/3.dtrace-ustack-helper</guid><dc:creator><![CDATA[Fedor Indutny]]></dc:creator><pubDate>Fri, 11 Jan 2013 00:00:00 GMT</pubDate></item><item><title><![CDATA[Candor returns]]></title><description><![CDATA[<p>Before I start diving into the deep sea of compiler internals, I would like to
familiarize you with the <a href="https://github.com/indutny/candor">Candor</a> programming language and its Virtual
Machine.</p>
<p>This is the thing I was working on last 10 months, and one of the most wonderful
and complex things I&#39;ve been working on since the start of my software
development career.</p>
<p>Candor is an Ecmascript-inspired language, but while the newer versions of the
Ecmascript standard are adding new functionality and syntax features, my
language aims to make the syntax as simple as possible.</p>
<h3 id="no-exceptions">No exceptions</h3>
<p>Caller can always be sure that function will return after the call. You should
either invoke a callback with an error argument, return negative number on
error, or do anything else to let caller know about errors that has happened.</p>
<h3 id="no-undefined-and-null">No undefined and null</h3>
<p>There is the only one value and type that represents undefined value - <code>nil</code>.
Thus, less checks and a more understandable behaviour of your application.</p>
<h3 id="no-implicit-global-variables">No implicit global variables</h3>
<p>Every global variable access should be done explicitly, by loading/storing
properties of the <code>global</code> object. To my mind, it&#39;s the most simplest and
powerful way to prevent global leaks.</p>
<h3 id="no-default-runtime">No default runtime</h3>
<p>Candor has no default APIs that are doing &#39;high-level&#39; things with objects and
arrays. These routines should be implemented by embedder (like <a href="https://github.com/indutny/candor.io">candor.io</a>).</p>
<p>Removing runtime from VM is good in terms of support, less dependencies - less
things to care about, and leaving things out of the core keeps it compact.</p>
<h3 id="no-prototype-chains">No prototype chains</h3>
<p>Objects are just magic-less hash-maps without special properties like
<code>toString</code> or <code>__proto__</code>. Additionally you can have both numeric and string
keys in objects (in other words, <code>a[0]</code> and <code>a[&#39;0&#39;]</code> are not the same thing).</p>
<p>Also there&#39;re no <code>length</code> property of array, it&#39;s replaced by <code>sizeof</code> keyword.
Example: <code>sizeof [1,2,3] == 3</code> or even <code>sizeof &quot;string&quot;</code>.</p>
<h3 id="no-complicated-type-coercion">No complicated type coercion</h3>
<p>Objects, arrays and nil are always converted either to empty string or to zero,
depending on type of another argument. For example, this lets you increment
uninitialized variables without getting any errors or unexpected behaviour:
<code>nil + 1 == 1</code>.</p>
<h3 id="dart-like-function-syntax">Dart-like function syntax</h3>
<p>No <code>function</code> keyword, yay! Just write:</p>
<pre><code>function_name(arguments) {
  //body
}
</code></pre><h2 id="syntax">Syntax</h2>
<p>You can learn more about syntax and play with it on <a href="http://candor-lang.org/">the official website</a>.</p>
<h2 id="compiler">Compiler</h2>
<p>Since the <a href="https://github.com/indutny/candor/commit/f3b1ebf3a839e32fcafa14b21af3">start of this year</a> I have been working on delivering very
primitive JIT compiler and VM for Candor. The first version was generating
pretty ugly machine code, which was ineffective and massive.</p>
<p>It was using the following algorithm:</p>
<ol>
<li>Visit AST node.</li>
<li>Generate all it&#39;s children, and place their results into <code>rax</code>, <code>rbx</code>, <code>rcx</code>
(depending on child&#39;s index). (Just in case - <a href="http://en.wikipedia.org/wiki/X86-64">x86-64</a>)</li>
<li>Generate code that calculates the result of operation and return value in
<code>rax</code>.</li>
</ol>
<p>Pros - fast compilation, easy to understand algorithm. Cons - hard way to deal
with different CPU architectures (i.e. it needed more than 6 registers), dumb
generated machine code.</p>
<p>Thanks to <a href="https://code.google.com/p/v8/">v8</a> and <a href="http://www.dartlang.org/">Dart</a> hacker <a href="http://mrale.ph/">Vyacheslav Egorov</a> and
<a href="http://wingolog.org/">Andy Wingo&#39;s blog</a>, I&#39;ve figured out that <a href="https://github.com/indutny/candor/wiki/Compiler-papers">there&#39;re much better ways</a> to
do JIT code generation, but it was too complex for me to understand at that
time. And despite I&#39;ve created new branch <code>feature-ssa</code> and written tons of
code, I&#39;ve never got something truly working.</p>
<p>I got stuck at implementing registry allocator, mostly because of wrong design
decisions that I made before, and continuing development of this branch in this
form was impossible.</p>
<p>That&#39;s why I took a long break (for almost 6 months) and worked on other
projects, until I realized how this thing should be implemented.</p>
<h2 id="candor-returns">Candor returns</h2>
<p>After this pause I&#39;ve considered many things and finally did it. Even more,
Candor now has two compilers: non-optimizing and optimizing. The non-optimizing
is used where it needs to compile a lot of source as fast as possible, and the
optimizing compiler is used for small functions that might be quickly optimized.</p>
<p>Main things that helped me to got to this state:</p>
<ol>
<li>Understanding how <a href="http://en.wikipedia.org/wiki/Control_flow_graph">CFG</a> and <a href="http://en.wikipedia.org/wiki/Static_single_assignment_form">SSA</a> should be really handled and
represented. CFG is a way to represent tree of input source code (AST) in a
linear form, by placing instructions in blocks and connecting them with the
control-flow edges like: goto and branch (which is used in <code>if</code> and <code>while</code>
statements). What I was missing is that the instruction and it&#39;s value should
be the same object, otherwise it&#39;s very problematic to exploit
<a href="http://en.wikipedia.org/wiki/Use-define_chain">def-use chains</a>, which are very useful for getting type information and
performing dead code elimination.</li>
<li>I was detecting variable conflicts in the blocks with two incoming
edges in a over-complicated way. I was using lists of active variables and
performing very complex analysis to propagate them to blocks that needed
them. Apparently, it&#39;s very cool and simple to do it in a way v8 does it. By
creating environment for each basic block in CFG, placing variables into it
and copying it as-it-is when adding successor to the block.</li>
<li>I didn&#39;t understand that low-level intermediate representation should
operate on <code>uses</code> which a parts of variable&#39;s liveness intervals... Previous
version was doing simplified linear-scan register allocation without holes in
variable&#39;s liveness intervals, which isn&#39;t resulting in good allocation.</li>
</ol>
<p>The main difference between optimizing and non-optimizing compiler is that the
former is trying to place everything in registers, while the latter operates
only on the stack slots (i.e. doing memory access on every variable load and
store).</p>
<p>By having a register allocator that&#39;s capable of allocating registers in very
generic terms, it was really straightforward to add support for a 32bit code
generation. And now Candor is officially running on two platforms: ia32 and x64.</p>
<h2 id="plans">Plans</h2>
<p>Now that there are two brand new compilers, I&#39;m going to work on adaptive
optimization/deoptimization for it. Candor should be capable of optimizing
hot functions on the fly and inlining small functions into their callers. Also,
it&#39;s quite practical to generate code that&#39;s very fast in common cases, and
falls back to unoptimized code in all other cases.</p>
<p>ARM support is also the part of my future plans for Candor, and I&#39;ll start 
working on it as soon as I&#39;ll receive my Raspberry PI.</p>
<h2 id="more-info">More info</h2>
<p>If you want to ask questions and/or learn more about Candor you can subscribe to
our <a href="https://groups.google.com/forum/?fromgroups&amp;hl=en#!forum/candorlang">google group</a> or join the #candor IRC channel on freenode.</p>
]]></description><link>http://darksi.de/2.candor-returns</link><guid isPermaLink="true">http://darksi.de/2.candor-returns</guid><dc:creator><![CDATA[Fedor Indutny]]></dc:creator><pubDate>Wed, 21 Nov 2012 00:00:00 GMT</pubDate></item><item><title><![CDATA[To lock, or not to lock]]></title><description><![CDATA[<h1 id="tl-dr">TL;DR</h1>
<p>As I&#39;ve promised you in my <a href="http://blog.indutny.com/0.benchmarking-tls">previous post</a>, I made <a href="https://github.com/indutny/tlsnappy">TLSnappy</a> balance and
handle requests a little bit better.</p>
<h1 id="data-flow">Data flow</h1>
<p>For leveraging all available CPUs TLSnappy runs multiple threads that are each
picking and processing tasks from their dispatch queues, one by one. Tasks are
created from node&#39;s event-loop in following cases:</p>
<ul>
<li>Data comes from client and should be decrypted</li>
<li>Data from server should be encrypted</li>
</ul>
<p>So, as you can see, each thread is receiving data from it&#39;s inputs (either
<code>encrypted</code> or <code>clear</code>) and/or emitting data to it&#39;s outputs. This pattern
apparently requires a lot of data transfer <code>to</code> and <code>from</code> worker threads and
requires storing (buffering) that data in some temporary storage before
processing it.</p>
<p>To my mind, best structure to fit this needs is <a href="http://en.wikipedia.org/wiki/Circular_buffer">Circular (Ring) buffer</a>.
Because it&#39;s fast, can be grown if more than it&#39;s current capacity needs to be
held.</p>
<p>The <a href="https://github.com/indutny/tlsnappy/blob/old-ring/src/ring.h">Naive version</a> of it was good enough to try out things, but it wasn&#39;t
supposed to be run in a multi-threaded concurrent environment - all access to
this buffer can take place only in a <a href="http://en.wikipedia.org/wiki/Critical_section">critical section</a>. This means that at
any time only one thread may access the ring&#39;s methods or properties. You might
think that this doesn&#39;t make difference, but, according to <a href="http://en.wikipedia.org/wiki/Amdahl&#39;s_law">Amdahl&#39;s law</a>,
reducing time spent in non-parallelizable (sequential) parts of application is
much more critical for overall performance than speeding up parallel parts.</p>
<h1 id="lock-less-ring-buffer">Lock-less ring buffer</h1>
<p>Removing locks seemed to be essential for achieving better performance, however
a special structure needs to be used in order to make a ring buffer work across
multiple CPUs. Here is the structure I chose for it:</p>
<p><img src="https://raw.github.com/indutny/tlsnappy/master/benchmark/ring.png" alt="Ring buffer"></p>
<p>Ring consists of pages that&#39;re forming circular linked list, each page has two
offsets: reader (<code>roffset</code>) and writer (<code>woffset</code>). And there&#39;re two special
pages (which could be the same one actually): reader head (<code>rhead</code>) and writer
head (<code>whead</code>).</p>
<p>Initially the ring contains only one page which is <code>rhead</code> and <code>whead</code> at the
same time. When the producer wants to put data in - it goes to the <code>whead</code>,
copies data into the page, increments <code>woffset</code> and if the page is full - it
create a new page, or reuses an old one that doesn&#39;t contain any un-read data.
Consumer takes <code>rhead</code> reads up to <code>woffset - roffset</code> bytes from it, increments
<code>roffset</code> and moves to the next page if <code>roffset</code> is equal to the size of the
page.</p>
<p>So here are benchmarks:</p>
<p>Without lock-less ring:</p>
<pre><code>Transactions:                 200000 hits
Availability:                 100.00 %
Elapsed time:                  47.90 secs
Data transferred:             585.37 MB
Response time:                  0.02 secs
Transaction rate:            4175.37 trans/sec
Throughput:                    12.22 MB/sec
Concurrency:                   98.79
Successful transactions:      200000
Failed transactions:               0
Longest transaction:            0.09
Shortest transaction:           0.00
</code></pre><p>With lock-less ring:</p>
<pre><code>Transactions:                 200000 hits
Availability:                 100.00 %
Elapsed time:                  47.37 secs
Data transferred:             585.37 MB
Response time:                  0.02 secs
Transaction rate:            4222.08 trans/sec
Throughput:                    12.36 MB/sec
Concurrency:                   98.83
Successful transactions:      200000
Failed transactions:               0
Longest transaction:            0.12
Shortest transaction:           0.00
</code></pre><p>As you can see, performance hasn&#39;t greatly improved and is actually almost
beyond statistical error (which means that results are nearly the same). However
these are results for small 3kb page, lets try sending some big 100kb buffers.</p>
<p>Without lock-less ring:</p>
<pre><code>Transactions:                 100000 hits
Availability:                 100.00 %
Elapsed time:                  64.06 secs
Data transferred:            9536.74 MB
Response time:                  0.06 secs
Transaction rate:            1561.04 trans/sec
Throughput:                   148.87 MB/sec
Concurrency:                   98.59
Successful transactions:      100000
Failed transactions:               0
Longest transaction:            1.93
Shortest transaction:           0.00
</code></pre><p>With lock-less ring:</p>
<pre><code>Transactions:                 100000 hits
Availability:                 100.00 %
Elapsed time:                  58.73 secs
Data transferred:            9536.74 MB
Response time:                  0.06 secs
Transaction rate:            1702.71 trans/sec
Throughput:                   162.38 MB/sec
Concurrency:                   98.98
Successful transactions:      100000
Failed transactions:               0
Longest transaction:            0.19
Shortest transaction:           0.00
</code></pre><p>Wow! That&#39;s much better - about 9% performance improvement.</p>
<h1 id="instruments">Instruments</h1>
<p>Still TLSnappy&#39;s performance wasn&#39;t even close to what nginx is capable of
(~5100 requests per second). Thus it was necessary to continue investigation and
this is where <a href="https://developer.apple.com/library/mac/#documentation/DeveloperTools/Conceptual/InstrumentsUserGuide/Introduction/Introduction.html">Instruments.app</a> comes into play, which is basically an UI for some
very useful dtrace scripts. I&#39;ve run the <code>CPU Sampler</code> utility and this is what
the call tree looked like:
<img src="https://raw.github.com/indutny/tlsnappy/master/benchmark/original-node.png" alt="Original node"></p>
<p>Obviously it spends almost 30% of time in synchronization between threads,
particularly in <code>CRYPTO_add_lock</code> function:
<img src="https://raw.github.com/indutny/tlsnappy/master/benchmark/old-crypto-add-lock.png" alt="Old CRYPTO_add_lock"></p>
<p>After modifying the code to use atomic operations, which are supported by almost
every CPU nowadays):
<img src="https://raw.github.com/indutny/tlsnappy/master/benchmark/new-crypto-add-lock.png" alt="New CRYPTO_add_lock"></p>
<p>Call tree locked like this:
<img src="https://raw.github.com/indutny/tlsnappy/master/benchmark/patched-node.png" alt="Patched node"></p>
<h1 id="results">Results</h1>
<p>I&#39;ve opened <a href="https://github.com/joyent/node/pull/4105">pull request for node.js</a> and sent the same patches to the
openssl-dev mailing list. With patched node and latest tlsnappy these are the
benchmark results:</p>
<p><img src="https://raw.github.com/indutny/tlsnappy/master/benchmark/tlsnappy-rps-2.png" alt="Requests per second">
<img src="https://raw.github.com/indutny/tlsnappy/master/benchmark/tlsnappy-load-2.png" alt="Average load"></p>
<p>And that&#39;s without patches:</p>
<p><img src="https://raw.github.com/indutny/tlsnappy/master/benchmark/tlsnappy-rps-siege.png" alt="Requests per second">
<img src="https://raw.github.com/indutny/tlsnappy/master/benchmark/tlsnappy-load-siege.png" alt="Average load"></p>
<p>A little comment about curve names here:</p>
<ul>
<li><code>default</code> - one tlsnappy process with 16 threads</li>
<li><code>hybrid</code> - 4 tlsnappy processes with 4 threads each</li>
<li><code>cluster</code> - 16 tlsnappy processes with 1 thread each</li>
<li><code>http</code> - 16 node.js processes in cluster</li>
</ul>
<p>The work is unfinished yet, but now I know that OpenSSL doesn&#39;t really behave
well when used in multithreaded application.</p>
]]></description><link>http://darksi.de/1.to-lock-or-not-to-lock</link><guid isPermaLink="true">http://darksi.de/1.to-lock-or-not-to-lock</guid><dc:creator><![CDATA[Fedor Indutny]]></dc:creator><pubDate>Thu, 11 Oct 2012 00:00:00 GMT</pubDate></item><item><title><![CDATA[Benchmarking TLS, TLSnappy and NGINX]]></title><description><![CDATA[<h1 id="tl-dr">TL;DR</h1>
<p>I&#39;ve created <a href="https://github.com/indutny/tlsnappy">TLSnappy</a> module which is going to be faster than internal TLS
module in node.js. So far it&#39;s slower on some benchmarks, but it&#39;ll definitely
be much snappier soon.</p>
<h1 id="preface">Preface</h1>
<p>Many people were complaining about <a href="http://nodejs.org/api/tls.html">tls</a> performance in node.js, which (as
they said) was significantly worse than in many other popular web servers,
balancers and terminators (i.e. nginx, haproxy..).</p>
<p>Several things were done to address this issue, including:</p>
<ul>
<li>Disabling OpenSSL compression in node:
<a href="http://journal.paul.querna.org/articles/2011/04/05/openssl-memory-use/">http://journal.paul.querna.org/articles/2011/04/05/openssl-memory-use/</a> and
<a href="https://github.com/joyent/node/commit/e83c695">https://github.com/joyent/node/commit/e83c695</a></li>
<li><a href="https://github.com/joyent/node/commit/e80cac62">Bundling a newer version of OpenSSL</a></li>
<li><a href="https://github.com/joyent/node/compare/7651228...e0e9f0c">Enabling inlined assembly</a></li>
<li><a href="https://github.com/joyent/node/commit/7651228">Using slab allocator to reduce memory allocation overhead</a></li>
</ul>
<p>After all that stuff got in, rps (requests per second) rate was significantly
improved, but many users were still unhappy with overall TLS performance.</p>
<h1 id="tlsnappy">TLSnappy</h1>
<p>This time, instead of patching and tweaking <a href="http://nodejs.org/api/tls.html">tls</a> I decided that it may be
worth trying to rewrite it from scratch as a third-party node.js addon. This
recently became <a href="https://github.com/TooTallNate/node-gyp/wiki/Linking-to-OpenSSL">possible</a>, thanks to <a href="https://github.com/TooTallNate">Nathan Rajlich</a> and his awesome
node.js native addon build tool <a href="https://github.com/TooTallNate/node-gyp">node-gyp</a>.</p>
<p>I didn&#39;t want to offer a module that&#39;s functionally equivalent to TLS, but
wanted to fix some issues (as I&#39;ve perceived them) and improve few things:</p>
<ul>
<li>Encryption/decryption should happen asynchronously (i.e. in other thread).
This could potentially speed up initial ssl handshake, and let the event loop
perform more operations while encryption/decryption is happening in the
background.</li>
<li>The builtin TLS module passes, slices and copies buffers in <a href="https://github.com/indutny/tlsnappy">javascript</a>.
All binary data operations should happen in C++.</li>
</ul>
<p>All this was implemented in <a href="https://github.com/indutny/tlsnappy">TLSnappy</a> module.</p>
<p>There were a lot of availability and stability issues (and surely much more that
I&#39;m yet unaware of). But tlsnappy seem to be quite a bit more performant than
the built-in tls module. Especially... when taking in account that <code>tlsnappy</code> is
by default using all available cores to encrypt/decrypt requests, while <code>tls</code>
module needs to be run in <a href="http://nodejs.org/api/cluster.html">cluster</a> to balance load between all cores.</p>
<h1 id="benchmarking">Benchmarking</h1>
<p>And I&#39;ve confirmed that when I was benchmaring it with Apache Benchmark (ab) on
my Macbook Pro and on dedicated Xeon server. Here a results from the latter one:</p>
<p><img src="https://raw.github.com/indutny/tlsnappy/master/benchmark/tlsnappy-rps.png" alt="Xeon 16 threads (rps) - Apache Benchmark">
<img src="https://raw.github.com/indutny/tlsnappy/master/benchmark/tlsnappy-ms.png" alt="Xeon 16 threads (ms) - Apache Benchmark"></p>
<p>A little comment about curve names here:</p>
<ul>
<li><code>default</code> - one tlsnappy process with 16 threads</li>
<li><code>hybrid</code> - 4 tlsnappy processes with 4 threads each</li>
<li><code>cluster</code> - 16 tlsnappy processes with 1 thread each</li>
<li><code>http</code> - 16 node.js processes in cluster</li>
</ul>
<p>As you can see tlsnappy is faster than tls server in almost every case, except
<code>cluster</code> mode (which just wasn&#39;t saturating CPU enough). Everything looked
great and shiny, until <a href="https://github.com/mranney">Matt Ranney</a> has pointed out that <code>ab</code> results of
https benchmarks are not really trustful:</p>
<p><blockquote class="twitter-tweet tw-align-center"><p>@<a href="https://twitter.com/ryah">ryah</a> @<a href="https://twitter.com/indutny">indutny</a> I was also mislead by &quot;ab&quot; with https benchmarks. I&#39;m not sure what tool to use instead though.</p>&mdash; Matt Ranney (@mranney) <a href="https://twitter.com/mranney/status/252137849468633088" data-datetime="2012-09-29T20:08:42+00:00">September 29, 2012</a></blockquote></p>
<script src="//platform.twitter.com/widgets.js" charset="utf-8" async></script>

<p>I&#39;ve installed siege, created node.js <a href="https://github.com/indutny/tlsnappy/blob/master/benchmark/script.js">script</a> and let it run for some time:</p>
<p><img src="https://raw.github.com/indutny/tlsnappy/master/benchmark/tlsnappy-rps-siege.png" alt="Xeon 16 threads (rps) - Siege"></p>
<p>Results are much better now (nginx was doing 5000 rps with siege and 2500 rps
with ab), but now tlsnappy seems to be slower than node.js&#39; default tls server.</p>
<p>I started investigation and decided to track not only rps rate, but a CPU load
too:</p>
<p><img src="https://raw.github.com/indutny/tlsnappy/master/benchmark/tlsnappy-load-siege.png" alt="Xeon 16 threads (load) - Siege"></p>
<h1 id="afterword">Afterword</h1>
<p>Right now, as you can see on the chart above, tlsnappy isn&#39;t saturating all CPUs
well. I suspect this is a major reason of its relative slowness in comparison
to both nginx and https module. I&#39;m working on making it balance and handle
requests better, and will sum up results of this investigation in the next blog
post.</p>
<p>For those of you, who are interested in more details -
<a href="https://docs.google.com/spreadsheet/ccc?key=0AhEDnA4M4EKGdDIwb3VYZTd1alA5T1pTVnlQWl9wanc">here is benchmarks&#39; data</a></p>
]]></description><link>http://darksi.de/0.benchmarking-tls</link><guid isPermaLink="true">http://darksi.de/0.benchmarking-tls</guid><dc:creator><![CDATA[Fedor Indutny]]></dc:creator><pubDate>Tue, 02 Oct 2012 00:00:00 GMT</pubDate></item></channel></rss>